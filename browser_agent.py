"""
æµè§ˆå™¨è‡ªåŠ¨åŒ–ä»£ç†
ä½¿ç”¨ Playwright + å¤šæ¨¡æ€æ¨¡å‹è‡ªåŠ¨æµè§ˆç½‘é¡µå¹¶æå–ä¿¡æ¯
åŸºäºè§†è§‰æ ‡æ³¨æ–¹æ¡ˆï¼ˆSet-of-Markï¼‰
"""
import asyncio
import base64
import json
import os
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Literal
import io

from playwright.async_api import async_playwright, Page, Browser
from pydantic import BaseModel, Field
import openai
import hashlib

# å¯¼å…¥PILç”¨äºå›¾åƒæ ‡æ³¨
try:
    from PIL import Image, ImageDraw, ImageFont
except ImportError:
    print("è­¦å‘Š: æœªå®‰è£…Pillowåº“ï¼Œè¯·è¿è¡Œ: pip install Pillow")
    Image = ImageDraw = ImageFont = None

logger = logging.getLogger(__name__)


# ========== ä»»åŠ¡ç®¡ç†å’Œè¿›åº¦è·Ÿè¸ª ==========

class SubTask(BaseModel):
    """å­ä»»åŠ¡å®šä¹‰ï¼ˆé¢å‘çŠ¶æ€çš„æ£€æŸ¥ç‚¹ï¼‰"""
    model_config = {"extra": "forbid"}

    id: int = Field(description="å­ä»»åŠ¡ç¼–å·")
    description: str = Field(
        description="å­ä»»åŠ¡çš„ç›®æ ‡çŠ¶æ€æè¿°ï¼ˆæè¿°æœŸæœ›è¾¾æˆçš„çŠ¶æ€ï¼Œè€Œä¸æ˜¯å…·ä½“æ“ä½œæ­¥éª¤ï¼‰"
    )
    success_criteria: List[str] = Field(
        description="å®Œæˆæ¡ä»¶åˆ—è¡¨ï¼ˆç”¨äºéªŒè¯ç›®æ ‡çŠ¶æ€æ˜¯å¦è¾¾æˆï¼‰ï¼Œä¾‹å¦‚ï¼š['å·²è·å¾—10æ¡å…¬å‘Šæ¸…å•', 'æ¯æ¡éƒ½æœ‰ä¸‹è½½é“¾æ¥']"
    )
    status: Literal["pending", "in_progress", "completed"] = Field(
        default="pending",
        description="å­ä»»åŠ¡çŠ¶æ€"
    )
    result: Optional[str] = Field(
        default=None,
        description="å­ä»»åŠ¡æ‰§è¡Œç»“æœ"
    )
    artifacts: List[str] = Field(
        default_factory=list,
        description="äº§å‡ºç‰©åˆ—è¡¨ï¼ˆå¦‚æ–‡ä»¶è·¯å¾„ã€URLã€æ•°æ®ç­‰ï¼‰ï¼Œç”¨äºè¿½è¸ªä»»åŠ¡äº§å‡º"
    )


class TaskDecomposition(BaseModel):
    """ä»»åŠ¡æ‹†è§£ç»“æœ"""
    model_config = {"extra": "forbid"}

    subtasks: List[SubTask] = Field(
        description="æ‹†è§£åçš„å­ä»»åŠ¡åˆ—è¡¨"
    )


class SubtaskCompletionCheck(BaseModel):
    """å­ä»»åŠ¡å®Œæˆæ£€æŸ¥ç»“æœ"""
    model_config = {"extra": "forbid"}

    completed: bool = Field(
        description="å­ä»»åŠ¡æ˜¯å¦å·²å®Œæˆ"
    )
    reason: str = Field(
        description="å®Œæˆæˆ–æœªå®Œæˆçš„åŸå› "
    )


class TaskManager:
    """ä»»åŠ¡æ‹†è§£å’Œè¿›åº¦ç®¡ç†"""

    def __init__(self, client: openai.OpenAI):
        self.client = client
        self.main_task: Optional[str] = None
        self.subtasks: List[SubTask] = []
        self.current_subtask_index: int = 0

    async def decompose_task(self, task_description: str) -> List[SubTask]:
        """
        ä½¿ç”¨ LLM å°†ä¸»ä»»åŠ¡æ‹†è§£ä¸ºå­ä»»åŠ¡

        ç¤ºä¾‹ï¼š
        è¾“å…¥: "æ‰¾åˆ°å¹¶é˜…è¯»300866è‚¡ç¥¨æœ€æ–°çš„10æ¡å…¬å‘Šï¼Œåˆ†ææ€»ç»“å…¶ä¸­å¯ä»¥ä¸ºæŠ•èµ„è€…æä¾›å“ªäº›æœªæ¥æŠ•èµ„çš„ä¾æ®"
        è¾“å‡º: [
            SubTask(id=0, description="å¯¼èˆªåˆ°æ·±åœ³è¯åˆ¸äº¤æ˜“æ‰€å…¬å‘ŠæŸ¥è¯¢é¡µé¢"),
            SubTask(id=1, description="æœç´¢è‚¡ç¥¨ä»£ç 300866"),
            SubTask(id=2, description="é˜…è¯»ç¬¬1æ¡å…¬å‘Šï¼ˆå®Œæ•´è¯»å–PDFå†…å®¹ï¼‰"),
            SubTask(id=3, description="è¿”å›åˆ—è¡¨é¡µ"),
            SubTask(id=4, description="é˜…è¯»ç¬¬2æ¡å…¬å‘Šï¼ˆå®Œæ•´è¯»å–PDFå†…å®¹ï¼‰"),
            ...
            SubTask(id=21, description="è¿”å›åˆ—è¡¨é¡µ"),
            SubTask(id=22, description="åˆ†ææ€»ç»“10æ¡å…¬å‘Šï¼Œæå–æŠ•èµ„ä¾æ®"),
        ]
        """
        self.main_task = task_description

        logger.info(f"ğŸ¯ å¼€å§‹æ‹†è§£ä»»åŠ¡: {task_description}")

        try:
            response = self.client.responses.parse(
                model="gpt-5-mini-2025-08-07",
                input=[
                    {
                        "role": "system",
                        "content": """ä½ æ˜¯ä¸€ä¸ªä»»åŠ¡è§„åˆ’ä¸“å®¶ã€‚è¯·å°†ç”¨æˆ·çš„ä»»åŠ¡æ‹†è§£ä¸º**é¢å‘çŠ¶æ€çš„æ£€æŸ¥ç‚¹**ï¼Œè€Œä¸æ˜¯å…·ä½“æ“ä½œæ­¥éª¤ã€‚

**æ ¸å¿ƒåŸåˆ™ï¼šæè¿°ç›®æ ‡çŠ¶æ€ï¼Œè€Œéæ“ä½œè¿‡ç¨‹**

âœ… æ­£ç¡®ç¤ºä¾‹ï¼ˆçŠ¶æ€å¯¼å‘ï¼‰ï¼š
- "å·²æ‰¾åˆ°å…¬å‘Šåˆ—è¡¨é¡µé¢"
- "å·²ä¸‹è½½æ‰€æœ‰éœ€è¦çš„PDFæ–‡ä»¶åˆ°æœ¬åœ°"
- "å·²å®Œæˆå†…å®¹åˆ†æå¹¶ç”ŸæˆæŠ¥å‘Š"

âŒ é”™è¯¯ç¤ºä¾‹ï¼ˆæ“ä½œå¯¼å‘ï¼‰ï¼š
- "åœ¨æœç´¢æ¡†è¾“å…¥300866" â† è¿™æ˜¯æ“ä½œæ­¥éª¤ï¼Œä¸æ˜¯çŠ¶æ€
- "ç‚¹å‡»æŸ¥è¯¢æŒ‰é’®" â† è¿‡äºå…·ä½“
- "æŒ‰æ—¥æœŸé™åºæ’åº" â† å‡è®¾äº†ç½‘ç«™ç»“æ„ï¼Œå¯èƒ½ä¸éœ€è¦

**æ‹†è§£è¦æ±‚**ï¼š
1. æ¯ä¸ªå­ä»»åŠ¡æè¿°ä¸€ä¸ª**æ˜ç¡®çš„ã€å¯éªŒè¯çš„çŠ¶æ€**ï¼ˆå·²å®Œæˆä»€ä¹ˆã€å·²è·å¾—ä»€ä¹ˆï¼‰
2. éªŒè¯æ¡ä»¶è¦**å®½æ¾å®ç”¨**ï¼Œä¸è¦è¿‡äºä¸¥è‹›ï¼Œå…è®¸éƒ¨åˆ†å®Œæˆåç»§ç»­
3. å­ä»»åŠ¡æ•°é‡æ§åˆ¶åœ¨ 2-3 ä¸ªï¼ˆç®€æ´ä¸ºä¸»ï¼‰
4. ä¸è¦å‡è®¾å…·ä½“çš„æ“ä½œæ­¥éª¤æˆ–ç½‘ç«™ç»“æ„
5. è®©æ‰§è¡ŒAgentè‡ªä¸»å†³å®šå¦‚ä½•è¾¾æˆç›®æ ‡çŠ¶æ€

**ç¤ºä¾‹ä»»åŠ¡**ï¼š"æ‰¾åˆ°å¹¶ä¸‹è½½300866è‚¡ç¥¨æœ€æ–°10æ¡å…¬å‘Šçš„PDF"

**æ­£ç¡®æ‹†è§£**ï¼š
```json
{
  "subtasks": [
    {
      "id": 1,
      "description": "å·²æ‰¾åˆ°300866çš„å…¬å‘Šåˆ—è¡¨é¡µé¢",
      "success_criteria": [
        "é¡µé¢URLæˆ–æ ‡é¢˜åŒ…å«'å…¬å‘Š'ã€'æŠ«éœ²'ç­‰å…³é”®è¯",
        "é¡µé¢ä¸­æœ‰å¤šæ¡å…¬å‘Šæ ‡é¢˜å’Œæ—¥æœŸ"
      ]
    },
    {
      "id": 2,
      "description": "å·²ä¸‹è½½è‡³å°‘5æ¡å…¬å‘ŠPDFåˆ°æœ¬åœ°",
      "success_criteria": [
        "æœ¬åœ°å­˜åœ¨è‡³å°‘5ä¸ªPDFæ–‡ä»¶",
        "æ–‡ä»¶ååŒ…å«å…¬å‘Šç›¸å…³ä¿¡æ¯"
      ]
    }
  ]
}
```

**é‡è¦**ï¼š
- éªŒè¯æ¡ä»¶ä¸è¦å¤ªæ­»æ¿ï¼ˆå¦‚"å¿…é¡»æ°å¥½10æ¡"ï¼‰ï¼Œå…è®¸"è‡³å°‘Næ¡"è¿™æ ·çš„å¼¹æ€§æ ‡å‡†
- ä¼˜å…ˆå…³æ³¨**æœ€ç»ˆå¯éªŒè¯çš„äº§ç‰©**ï¼ˆå¦‚æœ¬åœ°æ–‡ä»¶ï¼‰ï¼Œè€Œä¸æ˜¯ä¸­é—´çŠ¶æ€
- å¦‚æœæŸä¸ªçŠ¶æ€"å¤§æ¦‚ç‡æ­£ç¡®"å°±å¯ä»¥ç»§ç»­ï¼Œä¸è¦è¿‡åº¦éªŒè¯

è®°ä½ï¼šæè¿°"å·²è¾¾æˆçš„çŠ¶æ€"ï¼Œè€Œä¸æ˜¯"å¦‚ä½•è¾¾æˆ"ï¼ç›®æ ‡æ˜¯**å®ç”¨ã€å¯è¡Œ**ï¼Œè€Œä¸æ˜¯å®Œç¾ï¼"""
                    },
                    {
                        "role": "user",
                        "content": f"è¯·æ‹†è§£è¿™ä¸ªä»»åŠ¡ï¼š{task_description}"
                    }
                ],
                text_format=TaskDecomposition,
                max_output_tokens=3000,  # å¢åŠ ç©ºé—´ä»¥åŒ…å« success_criteria
            )

            # è·å–ç»“æ„åŒ–è¾“å‡º
            decomposition: TaskDecomposition = response.output_parsed
            self.subtasks = decomposition.subtasks
            logger.info(f"âœ… æˆåŠŸæ‹†è§£ä¸º {len(self.subtasks)} ä¸ªå­ä»»åŠ¡")
            return self.subtasks

        except Exception as e:
            logger.error(f"âŒ ä»»åŠ¡æ‹†è§£å¤±è´¥: {e}")
            # é™çº§ï¼šåˆ›å»ºä¸€ä¸ªç®€å•çš„å•ä»»åŠ¡
            self.subtasks = [SubTask(id=0, description=task_description)]
            return self.subtasks

    def get_current_subtask(self) -> Optional[SubTask]:
        """è·å–å½“å‰åº”è¯¥æ‰§è¡Œçš„å­ä»»åŠ¡"""
        if self.current_subtask_index < len(self.subtasks):
            return self.subtasks[self.current_subtask_index]
        return None

    def mark_current_subtask_complete(self, result: str = ""):
        """æ ‡è®°å½“å‰å­ä»»åŠ¡ä¸ºå®Œæˆï¼Œå¹¶ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ª"""
        if self.current_subtask_index < len(self.subtasks):
            self.subtasks[self.current_subtask_index].status = "completed"
            self.subtasks[self.current_subtask_index].result = result

            logger.info(f"âœ… å­ä»»åŠ¡ #{self.current_subtask_index} å®Œæˆ: {self.subtasks[self.current_subtask_index].description}")
            self.current_subtask_index += 1

    def mark_current_subtask_in_progress(self):
        """æ ‡è®°å½“å‰å­ä»»åŠ¡ä¸ºè¿›è¡Œä¸­"""
        if self.current_subtask_index < len(self.subtasks):
            self.subtasks[self.current_subtask_index].status = "in_progress"

    def get_progress_summary(self) -> str:
        """è·å–ä»»åŠ¡è¿›åº¦æ‘˜è¦"""
        total = len(self.subtasks)
        completed = sum(1 for task in self.subtasks if task.status == "completed")
        current = self.get_current_subtask()

        summary = f"**ä»»åŠ¡è¿›åº¦**: {completed}/{total} å·²å®Œæˆ\n"
        if current:
            summary += f"**å½“å‰ä»»åŠ¡**: #{current.id} - {current.description}\n"

        # åˆ—å‡ºæœ€è¿‘å®Œæˆçš„3ä¸ªå­ä»»åŠ¡
        completed_tasks = [t for t in self.subtasks if t.status == "completed"]
        if completed_tasks:
            summary += f"\n**æœ€è¿‘å®Œæˆ**:\n"
            for task in completed_tasks[-3:]:
                summary += f"  âœ… #{task.id}: {task.description}\n"

        # åˆ—å‡ºæ¥ä¸‹æ¥çš„2ä¸ªå­ä»»åŠ¡
        upcoming = self.subtasks[self.current_subtask_index + 1:self.current_subtask_index + 3]
        if upcoming:
            summary += f"\n**æ¥ä¸‹æ¥**:\n"
            for task in upcoming:
                summary += f"  â­ï¸ #{task.id}: {task.description}\n"

        return summary

    def is_all_complete(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ‰€æœ‰å­ä»»åŠ¡éƒ½å·²å®Œæˆ"""
        return self.current_subtask_index >= len(self.subtasks)

    def export_results(self) -> str:
        """å¯¼å‡ºæ‰€æœ‰å­ä»»åŠ¡çš„ç»“æœ"""
        output = f"# ä»»åŠ¡æ‰§è¡ŒæŠ¥å‘Š\n\n## ä¸»ä»»åŠ¡\n{self.main_task}\n\n## å­ä»»åŠ¡è¯¦æƒ…\n\n"
        for task in self.subtasks:
            output += f"### #{task.id}: {task.description}\n"
            output += f"**çŠ¶æ€**: {task.status}\n"
            if task.result:
                output += f"**ç»“æœ**: {task.result}\n"
            output += "\n"
        return output


# ========== é¡µé¢å…³ç³»å›¾è°± ==========

class PageNode(BaseModel):
    """é¡µé¢èŠ‚ç‚¹"""
    url: str = Field(description="é¡µé¢URL")
    page_type: Literal["entry", "list", "detail", "other"] = Field(
        default="other",
        description="é¡µé¢ç±»å‹"
    )
    title: Optional[str] = Field(default=None, description="é¡µé¢æ ‡é¢˜")
    description: Optional[str] = Field(default=None, description="é¡µé¢æè¿°")
    visited_count: int = Field(default=0, description="è®¿é—®æ¬¡æ•°")
    parent_url: Optional[str] = Field(default=None, description="çˆ¶é¡µé¢URLï¼ˆä»å“ªä¸ªé¡µé¢æ¥çš„ï¼‰")


class SiteGraph:
    """ç½‘ç«™é¡µé¢å…³ç³»å›¾è°±"""

    def __init__(self):
        self.nodes: Dict[str, PageNode] = {}
        self.current_url: Optional[str] = None
        self.navigation_history: List[str] = []  # URL å†å²æ ˆ

    def add_or_update_page(
        self,
        url: str,
        page_type: str = "other",
        title: str = None,
        description: str = None,
        parent_url: str = None
    ) -> PageNode:
        """æ·»åŠ æˆ–æ›´æ–°é¡µé¢èŠ‚ç‚¹"""
        if url in self.nodes:
            # æ›´æ–°å·²æœ‰èŠ‚ç‚¹
            node = self.nodes[url]
            node.visited_count += 1
            if title:
                node.title = title
            if description:
                node.description = description
        else:
            # åˆ›å»ºæ–°èŠ‚ç‚¹
            node = PageNode(
                url=url,
                page_type=page_type,
                title=title,
                description=description,
                parent_url=parent_url or self.current_url
            )
            self.nodes[url] = node
            logger.info(f"ğŸ—ºï¸  æ–°é¡µé¢: {page_type} - {url[:60]}")

        # æ›´æ–°å½“å‰ä½ç½®
        self.current_url = url

        # æ·»åŠ åˆ°å¯¼èˆªå†å²
        self.navigation_history.append(url)

        return node

    def get_current_page(self) -> Optional[PageNode]:
        """è·å–å½“å‰é¡µé¢èŠ‚ç‚¹"""
        if self.current_url:
            return self.nodes.get(self.current_url)
        return None

    def get_parent_page(self) -> Optional[PageNode]:
        """è·å–çˆ¶é¡µé¢ï¼ˆä¸Šä¸€çº§é¡µé¢ï¼‰"""
        current = self.get_current_page()
        if current and current.parent_url:
            return self.nodes.get(current.parent_url)
        return None

    def suggest_back_to_parent(self) -> str:
        """å»ºè®®å¦‚ä½•è¿”å›çˆ¶é¡µé¢"""
        parent = self.get_parent_page()
        if parent:
            return f"ğŸ’¡ **å¯¼èˆªæç¤º**: å½“å‰åœ¨è¯¦æƒ…é¡µï¼Œå®Œæˆé˜…è¯»åè¯·ä½¿ç”¨ BACK æ“ä½œè¿”å› {parent.page_type} é¡µï¼ˆ{parent.title or parent.url[:40]}ï¼‰ï¼Œä»¥ä¾¿ç»§ç»­ä¸‹ä¸€ä¸ªä»»åŠ¡"
        return ""

    def mark_navigation(self, from_url: str, to_url: str, action: str):
        """è®°å½•å¯¼èˆªè¡Œä¸ºï¼ˆæ–¹ä¾¿ç†è§£é¡µé¢å…³ç³»ï¼‰"""
        logger.info(f"ğŸ“ å¯¼èˆª: {action} - {from_url[:40]} â†’ {to_url[:40]}")

    def get_navigation_context(self) -> str:
        """è·å–å¯¼èˆªä¸Šä¸‹æ–‡ä¿¡æ¯"""
        current = self.get_current_page()
        if not current:
            return ""

        context = f"**å½“å‰é¡µé¢**: {current.page_type} - {current.title or current.url[:50]}\n"
        context += f"**è®¿é—®æ¬¡æ•°**: {current.visited_count}\n"

        # æ˜¾ç¤ºå¯¼èˆªè·¯å¾„ï¼ˆæœ€è¿‘5æ­¥ï¼‰
        if len(self.navigation_history) > 1:
            path = self.navigation_history[-5:]
            context += f"**å¯¼èˆªè·¯å¾„**: "
            for i, url in enumerate(path):
                node = self.nodes.get(url)
                if node:
                    context += f"{node.page_type}"
                    if i < len(path) - 1:
                        context += " â†’ "
            context += "\n"

        # å¦‚æœåœ¨è¯¦æƒ…é¡µï¼Œæç¤ºåº”è¯¥è¿”å›
        if current.page_type == "detail":
            parent = self.get_parent_page()
            if parent and parent.page_type == "list":
                context += f"\nğŸ’¡ **é‡è¦æç¤º**: ä½ ç°åœ¨åœ¨è¯¦æƒ…é¡µï¼Œå®Œæˆå½“å‰ä»»åŠ¡åï¼Œåº”ä½¿ç”¨ BACK è¿”å›åˆ—è¡¨é¡µç»§ç»­ä¸‹ä¸€ä¸ªä»»åŠ¡\n"

        return context


# ========== æ‰¹é‡æ‰§è¡Œå¼•æ“å’Œé¡µé¢ç¼“å­˜ ==========

class PageSnapshot(BaseModel):
    """é¡µé¢å¿«ç…§ï¼ˆç”¨äºç¼“å­˜å…ƒç´ ä¿¡æ¯ï¼‰"""
    model_config = {"extra": "forbid"}

    url: str
    timestamp: datetime
    elements: List[dict]
    html_hash: str  # ç”¨äºæ£€æµ‹é¡µé¢æ˜¯å¦å˜åŒ–

    def is_valid(self, current_url: str, current_html_hash: str, max_age_seconds: int = 300) -> bool:
        """æ£€æŸ¥å¿«ç…§æ˜¯å¦ä»ç„¶æœ‰æ•ˆ"""
        # URLå¿…é¡»åŒ¹é…
        if self.url != current_url:
            return False
        # HTMLå†…å®¹å¿…é¡»ç›¸åŒ
        if self.html_hash != current_html_hash:
            return False
        # ä¸èƒ½å¤ªæ—§ï¼ˆé»˜è®¤5åˆ†é’Ÿï¼‰
        age = (datetime.now() - self.timestamp).total_seconds()
        if age > max_age_seconds:
            return False
        return True


class BatchExecutionEngine:
    """æ‰¹é‡æ‰§è¡Œå¼•æ“ - æ”¯æŒæ–°æ ‡ç­¾é¡µæ¨¡å¼å’Œå…ƒç´ ç¼“å­˜"""

    def __init__(self):
        self.execution_count = 0

    async def execute_batch(
        self,
        context,  # Browser context
        list_page: Page,
        element_ids: List[int],
        description: str,
        use_new_tab: bool = True
    ) -> List[dict]:
        """
        æ‰¹é‡æ‰§è¡Œæ“ä½œåºåˆ—

        Args:
            context: Browser context (ç”¨äºåˆ›å»ºæ–°æ ‡ç­¾é¡µ)
            list_page: åˆ—è¡¨é¡µï¼ˆä¿æŒä¸å˜ï¼‰
            element_ids: è¦æ‰¹é‡æ“ä½œçš„å…ƒç´ IDåˆ—è¡¨
            description: æ“ä½œæè¿°
            use_new_tab: æ˜¯å¦ä½¿ç”¨æ–°æ ‡ç­¾é¡µæ¨¡å¼ï¼ˆæ¨èï¼‰

        Returns:
            æ‰§è¡Œç»“æœåˆ—è¡¨
        """
        logger.info(f"ğŸš€ å¼€å§‹æ‰¹é‡æ‰§è¡Œ: {description}")
        logger.info(f"   ç›®æ ‡å…ƒç´ : {element_ids}")
        logger.info(f"   æ¨¡å¼: {'æ–°æ ‡ç­¾é¡µ' if use_new_tab else 'å¯¼èˆªè¿”å›'}")

        results = []
        total = len(element_ids)

        for idx, element_id in enumerate(element_ids, start=1):
            logger.info(f"ğŸ“„ æ‰¹é‡å¤„ç† [{idx}/{total}]: å…ƒç´ #{element_id}")

            try:
                if use_new_tab:
                    # æ–°æ ‡ç­¾é¡µæ¨¡å¼ï¼šåˆ—è¡¨é¡µä¿æŒä¸å˜
                    result = await self._execute_in_new_tab(
                        context, list_page, element_id, idx, total
                    )
                else:
                    # å¯¼èˆªæ¨¡å¼ï¼šç‚¹å‡» â†’ æå– â†’ è¿”å›
                    result = await self._execute_with_navigation(
                        list_page, element_id, idx, total
                    )

                results.append({
                    "index": idx,
                    "element_id": element_id,
                    "status": "success",
                    "data": result
                })

            except Exception as e:
                logger.warning(f"âš ï¸  å…ƒç´ #{element_id}æ‰§è¡Œå¤±è´¥: {e}")
                results.append({
                    "index": idx,
                    "element_id": element_id,
                    "status": "failed",
                    "error": str(e)
                })

                # å¤±è´¥ç‡æ£€æŸ¥
                failure_rate = sum(1 for r in results if r["status"] == "failed") / len(results)
                if failure_rate > 0.3:
                    logger.error(f"âŒ æ‰¹é‡æ‰§è¡Œå¤±è´¥ç‡è¿‡é«˜({failure_rate:.0%})ï¼Œç»ˆæ­¢")
                    break

            # è½»å¾®å»¶è¿Ÿï¼Œé¿å…åçˆ¬
            await asyncio.sleep(0.5)

        success_count = sum(1 for r in results if r["status"] == "success")
        logger.info(f"âœ… æ‰¹é‡æ‰§è¡Œå®Œæˆ: {success_count}/{total} æˆåŠŸ")

        return results

    async def _execute_in_new_tab(
        self,
        context,
        list_page: Page,
        element_id: int,
        index: int,  # noqa: ARG002 - ä¿ç•™ç”¨äºæ—¥å¿—
        total: int   # noqa: ARG002 - ä¿ç•™ç”¨äºæ—¥å¿—
    ) -> dict:
        """åœ¨æ–°æ ‡ç­¾é¡µä¸­æ‰§è¡Œï¼ˆåˆ—è¡¨é¡µä¿æŒä¸å˜ï¼‰"""
        detail_page = None

        try:
            # è·å–ç›®æ ‡å…ƒç´ çš„href
            selector = f'[data-browser-agent-id="ba-{element_id}"]'
            href = await list_page.get_attribute(selector, 'href')

            if href:
                # æ–¹å¼1: ç›´æ¥åœ¨æ–°æ ‡ç­¾é¡µæ‰“å¼€URL
                detail_page = await context.new_page()
                # ç¡®ä¿URLæ˜¯ç»å¯¹è·¯å¾„
                if not href.startswith('http'):
                    from urllib.parse import urljoin
                    href = urljoin(list_page.url, href)
                await detail_page.goto(href, wait_until='networkidle', timeout=15000)
            else:
                # æ–¹å¼2: ä½¿ç”¨Ctrl+Clickåœ¨æ–°æ ‡ç­¾é¡µæ‰“å¼€
                async with context.expect_page() as new_page_info:
                    # Macç”¨Meta(Command)ï¼ŒWindows/Linuxç”¨Control
                    await list_page.click(selector, modifiers=['Meta'])
                detail_page = await new_page_info.value
                await detail_page.wait_for_load_state('networkidle', timeout=15000)

            # æå–å†…å®¹
            title = await detail_page.title()
            content = await detail_page.text_content('body')

            # æ£€æŸ¥æ˜¯å¦æœ‰PDFé“¾æ¥
            pdf_links = await detail_page.query_selector_all('a[href$=".pdf"]')
            pdf_urls = []
            for link in pdf_links[:5]:  # æœ€å¤šè®°å½•5ä¸ªPDF
                url = await link.get_attribute('href')
                if url:
                    pdf_urls.append(url)

            # å…³é—­è¯¦æƒ…é¡µ
            await detail_page.close()

            return {
                "title": title,
                "content": content[:1000],  # ä¿å­˜å‰1000å­—ç¬¦
                "pdf_urls": pdf_urls,
                "url": href or detail_page.url
            }

        except Exception as e:
            logger.warning(f"æ–°æ ‡ç­¾é¡µæ‰§è¡Œå¤±è´¥: {e}")
            if detail_page:
                await detail_page.close()
            raise

    async def _execute_with_navigation(
        self,
        page: Page,
        element_id: int,
        index: int,  # noqa: ARG002 - ä¿ç•™ç”¨äºæ—¥å¿—
        total: int   # noqa: ARG002 - ä¿ç•™ç”¨äºæ—¥å¿—
    ) -> dict:
        """ä½¿ç”¨å¯¼èˆªæ¨¡å¼æ‰§è¡Œï¼ˆç‚¹å‡» â†’ æå– â†’ è¿”å›ï¼‰"""
        try:
            # ç‚¹å‡»å…ƒç´ 
            selector = f'[data-browser-agent-id="ba-{element_id}"]'
            await page.click(selector)
            await page.wait_for_load_state('networkidle', timeout=10000)

            # éªŒè¯é¡µé¢åŠ è½½æˆåŠŸ
            title = await page.title()
            if "404" in title or "é”™è¯¯" in title:
                raise Exception(f"é¡µé¢åŠ è½½å¤±è´¥: {title}")

            # æå–å†…å®¹
            content = await page.text_content('body')

            # æ£€æŸ¥PDFé“¾æ¥
            pdf_links = await page.query_selector_all('a[href$=".pdf"]')
            pdf_urls = []
            for link in pdf_links[:5]:
                url = await link.get_attribute('href')
                if url:
                    pdf_urls.append(url)

            # è¿”å›åˆ—è¡¨é¡µï¼ˆå…³é”®ï¼šè¿™é‡Œä¸é‡æ–°æå–å…ƒç´ ï¼Œä½¿ç”¨ç¼“å­˜ï¼‰
            await page.go_back()
            await page.wait_for_load_state('networkidle', timeout=5000)

            return {
                "title": title,
                "content": content[:1000],
                "pdf_urls": pdf_urls,
                "url": page.url
            }

        except Exception as e:
            logger.warning(f"å¯¼èˆªæ¨¡å¼æ‰§è¡Œå¤±è´¥: {e}")
            # å°è¯•è¿”å›
            try:
                await page.go_back()
                await page.wait_for_load_state('networkidle', timeout=5000)
            except:
                pass
            raise


# å®šä¹‰å†³ç­–æ¨¡å‹ï¼ˆStructured Output - åŸºäºå…ƒç´ IDçš„è§†è§‰æ ‡æ³¨æ–¹æ¡ˆï¼‰
class BrowserDecision(BaseModel):
    """æµè§ˆå™¨æ“ä½œå†³ç­–ï¼ˆè§†è§‰æ ‡æ³¨æ–¹æ¡ˆï¼‰"""
    model_config = {"extra": "forbid"}

    action: Literal["CLICK", "TYPE", "SCROLL", "BACK", "FORWARD", "REFRESH", "TASK_COMPLETE", "BATCH_EXECUTE", "CHECK_DOWNLOADS"] = Field(
        description="è¦æ‰§è¡Œçš„æ“ä½œç±»å‹"
    )
    reasoning: str = Field(
        description="ä¸ºä»€ä¹ˆé€‰æ‹©è¿™ä¸ªæ“ä½œçš„åŸå› "
    )
    element_id: Optional[int] = Field(
        default=None,
        description="è¦æ“ä½œçš„å…ƒç´ ç¼–å·ï¼ˆä»æ ‡æ³¨çš„æˆªå›¾ä¸­é€‰æ‹©ï¼Œç”¨äºCLICKå’ŒTYPEï¼‰"
    )
    text: Optional[str] = Field(
        default=None,
        description="è¦è¾“å…¥çš„æ–‡æœ¬å†…å®¹ï¼ˆç”¨äºTYPEæ“ä½œï¼‰"
    )
    scroll_amount: Optional[int] = Field(
        default=500,
        description="æ»šåŠ¨è·ç¦»ï¼ˆåƒç´ ï¼‰ï¼Œæ­£æ•°å‘ä¸‹ï¼Œè´Ÿæ•°å‘ä¸Šï¼ˆç”¨äºSCROLLæ“ä½œï¼‰"
    )
    summary: Optional[str] = Field(
        default=None,
        description="ä»»åŠ¡å®Œæˆæ—¶çš„ç­”æ¡ˆæ‘˜è¦ï¼Œ100-200å­—ï¼ˆä»…ç”¨äºTASK_COMPLETEï¼‰"
    )
    citations: Optional[List[str]] = Field(
        default=None,
        description="éœ€è¦åœ¨æŠ¥å‘Šä¸­é«˜äº®æ˜¾ç¤ºçš„åŸæ–‡å¼•ç”¨ç‰‡æ®µåˆ—è¡¨ï¼Œæ¯ä¸ª50-200å­—ï¼ˆä»…ç”¨äºTASK_COMPLETEï¼‰"
    )
    # æ‰¹é‡æ‰§è¡Œç›¸å…³å­—æ®µ
    batch_element_ids: Optional[List[int]] = Field(
        default=None,
        description="æ‰¹é‡æ‰§è¡Œçš„ç›®æ ‡å…ƒç´ IDåˆ—è¡¨ï¼ˆç”¨äºBATCH_EXECUTEï¼‰"
    )
    batch_description: Optional[str] = Field(
        default=None,
        description="æ‰¹é‡æ“ä½œçš„æè¿°ï¼Œå¦‚'ç‚¹å‡»å…¬å‘Šå¹¶æå–å†…å®¹'ï¼ˆç”¨äºBATCH_EXECUTEï¼‰"
    )

class BrowserAgent:
    """æµè§ˆå™¨è‡ªåŠ¨åŒ–ä»£ç†"""

    def __init__(self, api_key: str, max_steps: int = 10, headless: bool = True, slow_mo: int = 0):
        """
        åˆå§‹åŒ–æµè§ˆå™¨ä»£ç†

        Args:
            api_key: OpenAI API Key
            max_steps: æœ€å¤§æ“ä½œæ­¥æ•°
            headless: æ˜¯å¦ä½¿ç”¨æ— å¤´æ¨¡å¼ï¼ˆFalse=å¯ä»¥çœ‹åˆ°æµè§ˆå™¨çª—å£ï¼‰
            slow_mo: æ¯ä¸ªæ“ä½œå»¶è¿Ÿæ¯«ç§’æ•°ï¼ˆä¾¿äºè§‚å¯Ÿï¼Œå»ºè®®1000-2000ï¼‰
        """
        self.api_key = api_key
        self.max_steps = max_steps
        self.headless = headless
        self.slow_mo = slow_mo
        self.client = openai.OpenAI(api_key=api_key)

        # åˆ›å»ºæˆªå›¾å­˜å‚¨ç›®å½•
        self.screenshots_dir = Path("task_data/screenshots")
        self.screenshots_dir.mkdir(parents=True, exist_ok=True)

        # åˆ›å»ºæŠ¥å‘Šå­˜å‚¨ç›®å½•
        self.reports_dir = Path("task_data/reports")
        self.reports_dir.mkdir(parents=True, exist_ok=True)

        # å¯¹è¯å†å²ç®¡ç† - ç”¨äºå¤šè½®å¯¹è¯å’Œ prompt caching
        self.conversation_history = []
        self.static_system_prompt = None  # é™æ€ system promptï¼ˆå¯ç¼“å­˜ï¼‰
        self.current_query = None  # å½“å‰ä»»åŠ¡æŸ¥è¯¢
        self.history_summary = None  # å†å²æ€»ç»“ï¼ˆå½“å¯¹è¯è¶…è¿‡é˜ˆå€¼æ—¶ï¼‰
        self.max_history_messages = 20  # å¯¹è¯å†å²æœ€å¤§æ¶ˆæ¯æ•°

        # é‡å¤æ“ä½œæ£€æµ‹
        self.recent_actions = []  # æœ€è¿‘çš„æ“ä½œè®°å½• [(action, element_id, text), ...]
        self.max_recent_actions = 5  # ä¿ç•™æœ€è¿‘5ä¸ªæ“ä½œç”¨äºæ£€æµ‹é‡å¤

        # ä»»åŠ¡ç®¡ç†å’Œé¡µé¢å›¾è°± ğŸ†•
        self.task_manager = TaskManager(self.client)
        self.site_graph = SiteGraph()

        # æ‰¹é‡æ‰§è¡Œå¼•æ“å’Œé¡µé¢ç¼“å­˜ ğŸ†•
        self.batch_engine = BatchExecutionEngine()
        self.page_cache: Dict[str, PageSnapshot] = {}  # URL -> å¿«ç…§

    def _init_conversation(self, query: str):
        """
        åˆå§‹åŒ–å¯¹è¯å†å²ï¼ˆæ¯ä¸ªæ–°ä»»åŠ¡æ—¶è°ƒç”¨ï¼‰

        Args:
            query: ç”¨æˆ·é—®é¢˜
        """
        self.current_query = query
        self.conversation_history = []
        self.history_summary = None  # é‡ç½®å†å²æ€»ç»“
        self.recent_actions = []  # é‡ç½®æœ€è¿‘æ“ä½œè®°å½•

        # é‡ç½®ä»»åŠ¡ç®¡ç†å’Œé¡µé¢å›¾è°± ğŸ†•
        self.task_manager = TaskManager(self.client)
        self.site_graph = SiteGraph()

        # åˆ›å»ºé™æ€ system promptï¼ˆä¸åŒ…å«åŠ¨æ€ä¿¡æ¯ï¼Œä¾¿äº prompt cachingï¼‰
        self.static_system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç½‘é¡µæµè§ˆåŠ©æ‰‹ï¼Œé€šè¿‡åˆ†ææ ‡æ³¨åçš„é¡µé¢æˆªå›¾æ¥æ“ä½œæµè§ˆå™¨ã€‚

æˆªå›¾è¯´æ˜ï¼š
- çº¢è‰²è¾¹æ¡†æ ‡æ³¨äº†æ‰€æœ‰å¯äº¤äº’å…ƒç´ 
- æ¯ä¸ªå…ƒç´ éƒ½æœ‰ç¼–å· [1], [2], [3] ç­‰
- è¯·ç›´æ¥ä»æˆªå›¾ä¸­è¯†åˆ«å…ƒç´ çš„ä½ç½®ã€ç±»å‹å’Œæ–‡æœ¬å†…å®¹
- åªéœ€è¿”å›å…ƒç´ ç¼–å·ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨æ ¹æ®ç¼–å·å®šä½å’Œæ“ä½œå…ƒç´ 

å¯ç”¨æ“ä½œï¼š

1. CLICK - ç‚¹å‡»æŸä¸ªå…ƒç´ 
   - éœ€è¦æä¾›: element_idï¼ˆå…ƒç´ ç¼–å·ï¼‰
   - ç”¨é€”: ç‚¹å‡»é“¾æ¥ã€æŒ‰é’®ã€èœå•ç­‰
   - ç¤ºä¾‹: ç‚¹å‡»æœç´¢æŒ‰é’®ï¼Œé€‰æ‹©element_id=2

2. TYPE - åœ¨è¾“å…¥æ¡†è¾“å…¥æ–‡æœ¬
   - éœ€è¦æä¾›: element_idï¼ˆè¾“å…¥æ¡†ç¼–å·ï¼‰, textï¼ˆè¦è¾“å…¥çš„å†…å®¹ï¼‰
   - ç”¨é€”: åœ¨æœç´¢æ¡†ã€è¡¨å•ç­‰è¾“å…¥ä¿¡æ¯
   - æ³¨æ„: ä¼šè‡ªåŠ¨åœ¨è¾“å…¥æ¡†ä¸­è¾“å…¥æ–‡æœ¬å¹¶æŒ‰å›è½¦
   - ç¤ºä¾‹: åœ¨æœç´¢æ¡†è¾“å…¥"ä¸­å›½äººå£"ï¼Œé€‰æ‹©element_id=1, text="ä¸­å›½äººå£"

3. SCROLL - æ»šåŠ¨é¡µé¢æˆ–å…ƒç´ 
   - éœ€è¦æä¾›: scroll_amountï¼ˆæ»šåŠ¨è·ç¦»ï¼Œåƒç´ ï¼‰
   - å¯é€‰æä¾›: element_idï¼ˆè¦æ»šåŠ¨çš„å…ƒç´ ç¼–å·ï¼‰
   - æ­£æ•°å‘ä¸‹æ»šåŠ¨ï¼Œè´Ÿæ•°å‘ä¸Šæ»šåŠ¨ï¼Œé»˜è®¤500px
   - ç”¨é€”:
     * ä¸æŒ‡å®šelement_idï¼šæ»šåŠ¨æ•´ä¸ªé¡µé¢ï¼ˆé»˜è®¤ï¼‰
     * æŒ‡å®šelement_idï¼šæ»šåŠ¨ç‰¹å®šå…ƒç´ ï¼ˆå¦‚æ–‡æ¡£æŸ¥çœ‹å™¨ã€iframeã€å¯æ»šåŠ¨divï¼‰
   - ç¤ºä¾‹: æ»šåŠ¨é¡µé¢ç”¨scroll_amount=500ï¼Œæ»šåŠ¨å…ƒç´ 5ç”¨element_id=5, scroll_amount=300

4. BACK - æµè§ˆå™¨åé€€
   - ä¸éœ€è¦é¢å¤–å‚æ•°
   - ç”¨é€”: è¿”å›ä¸Šä¸€ä¸ªé¡µé¢
   - ç¤ºä¾‹: ç‚¹å‡»è¿›å…¥äº†é”™è¯¯çš„é¡µé¢ï¼Œå¯ä»¥åé€€é‡æ–°é€‰æ‹©

5. FORWARD - æµè§ˆå™¨å‰è¿›
   - ä¸éœ€è¦é¢å¤–å‚æ•°
   - ç”¨é€”: å‰è¿›åˆ°ä¸‹ä¸€ä¸ªé¡µé¢ï¼ˆåœ¨ä½¿ç”¨åé€€åï¼‰

6. REFRESH - åˆ·æ–°é¡µé¢
   - ä¸éœ€è¦é¢å¤–å‚æ•°
   - ç”¨é€”: é‡æ–°åŠ è½½å½“å‰é¡µé¢
   - ç¤ºä¾‹: é¡µé¢åŠ è½½ä¸å®Œæ•´æˆ–éœ€è¦æ›´æ–°æ•°æ®æ—¶ä½¿ç”¨

7. BATCH_EXECUTE - æ‰¹é‡æ‰§è¡Œç›¸åŒæ“ä½œï¼ˆâš¡ é«˜æ•ˆæ¨¡å¼ï¼‰
   - éœ€è¦æä¾›: batch_element_idsï¼ˆå…ƒç´ IDåˆ—è¡¨ï¼‰, batch_descriptionï¼ˆæ“ä½œæè¿°ï¼‰
   - ç”¨é€”: å½“éœ€è¦å¯¹å¤šä¸ªç›¸ä¼¼å…ƒç´ æ‰§è¡Œç›¸åŒæ“ä½œæ—¶ï¼ˆå¦‚æ‰¹é‡é˜…è¯»å…¬å‘Šï¼‰
   - ä½¿ç”¨æ—¶æœºï¼š
     * âœ… ç¬¬1æ¬¡å•ç‹¬æ‰§è¡ŒæˆåŠŸï¼ŒéªŒè¯äº†æ“ä½œå¯è¡Œ
     * âœ… æ‰€æœ‰ç›®æ ‡å…ƒç´ ç»“æ„ç›¸ä¼¼ï¼ˆå¦‚åˆ—è¡¨ä¸­çš„å¤šä¸ªé“¾æ¥ï¼‰
     * âœ… æ“ä½œæµç¨‹å›ºå®šï¼ˆç‚¹å‡»â†’æå–å†…å®¹ï¼‰
     * âœ… æ²¡æœ‰åçˆ¬è™«ã€éªŒè¯ç ç­‰éšœç¢
   - ç¤ºä¾‹:
     ```json
     {
       "action": "BATCH_EXECUTE",
       "batch_element_ids": [12, 19, 26, 33, 40],
       "batch_description": "ç‚¹å‡»å…¬å‘Šé“¾æ¥å¹¶æå–å†…å®¹",
       "reasoning": "å·²éªŒè¯ç¬¬1ä¸ªå…¬å‘Šè¯»å–æˆåŠŸï¼Œå‰©ä½™4ä¸ªå…¬å‘Šç»“æ„ç›¸åŒï¼Œä½¿ç”¨æ‰¹é‡æ‰§è¡Œæå‡æ•ˆç‡"
     }
     ```
   - âš¡ ä¼˜åŠ¿: è‡ªåŠ¨ä½¿ç”¨æ–°æ ‡ç­¾é¡µæ¨¡å¼ï¼Œåˆ—è¡¨é¡µä¿æŒä¸å˜ï¼Œæ— éœ€é‡å¤æå–å…ƒç´ ï¼Œå¤§å¹…èŠ‚çœæ—¶é—´
   - âš ï¸ æ³¨æ„: åªæœ‰åœ¨ç¬¬1æ¬¡å•ç‹¬æ“ä½œæˆåŠŸåæ‰ä½¿ç”¨ï¼Œç¡®ä¿æµç¨‹å¯è¡Œ

8. TASK_COMPLETE - ä»»åŠ¡å®Œæˆ
   - éœ€è¦æä¾›: summaryï¼ˆç­”æ¡ˆæ‘˜è¦ï¼Œ100-200å­—ï¼‰, citationsï¼ˆå¼•ç”¨ç‰‡æ®µåˆ—è¡¨ï¼‰
   - ç”¨é€”: å½“æ‰¾åˆ°é—®é¢˜çš„ç­”æ¡ˆæ—¶ä½¿ç”¨

9. CHECK_DOWNLOADS - æŸ¥çœ‹å·²ä¸‹è½½æ–‡ä»¶ (ğŸ“¥ æ–‡ä»¶ç³»ç»Ÿè®¿é—®)
   - ä¸éœ€è¦é¢å¤–å‚æ•°
   - ç”¨é€”: æŸ¥çœ‹downloadsç›®å½•ä¸­å·²ä¸‹è½½çš„æ–‡ä»¶åˆ—è¡¨
   - è¿”å›: æ–‡ä»¶åã€å¤§å°ã€ä¸‹è½½æ—¶é—´ç­‰ä¿¡æ¯
   - ä½¿ç”¨åœºæ™¯:
     * ç¡®è®¤PDF/æ–‡æ¡£æ˜¯å¦å·²æˆåŠŸä¸‹è½½
     * éªŒè¯æ–‡ä»¶åå’Œæ–‡ä»¶å¤§å°
     * æ£€æŸ¥ä¸‹è½½çš„æ–‡ä»¶æ•°é‡
   - ç¤ºä¾‹: å½“ç‚¹å‡»ä¸‹è½½æŒ‰é’®åï¼Œä½¿ç”¨CHECK_DOWNLOADSç¡®è®¤æ–‡ä»¶æ˜¯å¦å·²ä¿å­˜åˆ°æœ¬åœ°

é‡è¦æç¤ºï¼š
- è¯·æ ¹æ®æˆªå›¾çš„è§†è§‰ä¿¡æ¯åšå†³ç­–ï¼ˆæˆªå›¾ä¸­çš„å…ƒç´ å·²ç”¨çº¢æ¡†å’Œç¼–å·æ ‡æ³¨ï¼‰
- è¿”å›å…ƒç´ ç¼–å·ï¼ˆelement_idï¼‰ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨æ ¹æ®ç¼–å·å®šä½å’Œæ“ä½œå…ƒç´ 
- å¦‚æœéœ€è¦æœç´¢ï¼Œç›´æ¥ç”¨TYPEåœ¨æœç´¢æ¡†è¾“å…¥ï¼Œä¸éœ€è¦å†ç‚¹å‡»æœç´¢æŒ‰é’®
- æ¯ä¸ªæ“ä½œéƒ½å¿…é¡»æä¾›reasoningå­—æ®µè¯´æ˜åŸå› 
- ä½ å¯ä»¥çœ‹åˆ°ä¹‹å‰çš„æ“ä½œå†å²å’Œç»“æœï¼Œè¯·æ ¹æ®å†å²ä¿¡æ¯åšå‡ºæ›´å¥½çš„å†³ç­–

**æµè§ˆå™¨æ§åˆ¶æ“ä½œçš„ä½¿ç”¨å»ºè®®**ï¼š
- ğŸ”™ BACKï¼ˆåé€€ï¼‰ï¼šå½“è¿›å…¥é”™è¯¯é¡µé¢ã€æ­»èƒ¡åŒã€æˆ–éœ€è¦è¿”å›ä¸Šçº§é¡µé¢æ—¶ä½¿ç”¨
- ğŸ”œ FORWARDï¼ˆå‰è¿›ï¼‰ï¼šåœ¨ä½¿ç”¨åé€€åï¼Œå¦‚æœéœ€è¦è¿”å›ä¹‹å‰è®¿é—®çš„é¡µé¢
- ğŸ”„ REFRESHï¼ˆåˆ·æ–°ï¼‰ï¼šé¡µé¢åŠ è½½ä¸å®Œæ•´ã€æ•°æ®éœ€è¦æ›´æ–°ã€æˆ–å‡ºç°é”™è¯¯æ—¶ä½¿ç”¨
- è¿™äº›æ“ä½œå¯ä»¥æœ‰æ•ˆé¿å…é™·å…¥å›°å¢ƒï¼Œæé«˜ä»»åŠ¡æˆåŠŸç‡

**é¿å…é‡å¤æ“ä½œçš„ç­–ç•¥**ï¼š
- âš ï¸ å¦‚æœæŸä¸ªæ“ä½œå·²ç»æ‰§è¡Œè¿‡ä½†æ²¡æœ‰äº§ç”Ÿé¢„æœŸæ•ˆæœï¼Œä¸è¦é‡å¤æ‰§è¡Œ
- âš ï¸ å¦‚æœæ”¶åˆ°é‡å¤æ“ä½œè­¦å‘Šï¼Œå¿…é¡»ç«‹å³æ”¹å˜ç­–ç•¥
- âš ï¸ é‡å¤ç‚¹å‡»åŒä¸€ä¸ªå…ƒç´ æ˜¯æ— æ•ˆçš„ï¼Œè¯·å°è¯•å…¶ä»–å…ƒç´ æˆ–æ“ä½œ
- å»ºè®®ç­–ç•¥ï¼šä½¿ç”¨BACKè¿”å›ä¸Šä¸€é¡µã€SCROLLæŸ¥çœ‹æ›´å¤šå…ƒç´ ã€ç‚¹å‡»å…¶ä»–ç›¸å…³é“¾æ¥ã€æˆ–é‡æ–°æ€è€ƒä»»åŠ¡ç›®æ ‡"""

    async def _summarize_conversation_history(self) -> str:
        """
        ä½¿ç”¨ LLM æ€»ç»“å¯¹è¯å†å²

        Returns:
            å†å²æ€»ç»“æ–‡æœ¬
        """
        logger.info("å¼€å§‹æ€»ç»“å¯¹è¯å†å²...")

        # æ„å»ºå†å²æ–‡æœ¬
        history_text = ""
        for msg in self.conversation_history:
            role = msg['role']
            content = msg['content']

            # å¤„ç†ä¸åŒç±»å‹çš„content
            if isinstance(content, str):
                history_text += f"{role.upper()}: {content}\n\n"
            elif isinstance(content, list):
                # åªæå–æ–‡æœ¬éƒ¨åˆ†
                for item in content:
                    if isinstance(item, dict) and item.get('type') == 'input_text':
                        history_text += f"{role.upper()}: {item.get('text', '')}\n\n"

        # è°ƒç”¨ LLM æ€»ç»“
        try:
            response = self.client.chat.completions.create(
                model="gpt-5-mini-2025-08-07",  
                messages=[
                    {
                        "role": "system",
                        "content": """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å¯¹è¯æ€»ç»“åŠ©æ‰‹ã€‚è¯·æ€»ç»“æµè§ˆå™¨è‡ªåŠ¨åŒ–ä»»åŠ¡çš„å¯¹è¯å†å²ã€‚

æ€»ç»“è¦æ±‚ï¼š
1. ç®€æ˜æ‰¼è¦ï¼Œé‡ç‚¹å…³æ³¨ï¼šå·²æ‰§è¡Œçš„æ“ä½œã€è®¿é—®çš„é¡µé¢ã€é‡åˆ°çš„é—®é¢˜
2. ä½¿ç”¨é¡¹ç›®ç¬¦å·åˆ—è¡¨æ ¼å¼
3. ä¿ç•™å…³é”®ä¿¡æ¯ï¼šURLã€æ“ä½œç±»å‹ã€é‡è¦çš„æ¨ç†ä¾æ®
4. å¿½ç•¥æŠ€æœ¯ç»†èŠ‚å’Œå…ƒç´ ID
5. é•¿åº¦æ§åˆ¶åœ¨200-300å­—ä»¥å†…

ç¤ºä¾‹æ ¼å¼ï¼š
**å·²æ‰§è¡Œæ“ä½œ**ï¼š
- æ­¥éª¤1ï¼šåœ¨æœç´¢æ¡†è¾“å…¥"é¢„åˆ¶èœå®šä¹‰"
- æ­¥éª¤2ï¼šç‚¹å‡»æœç´¢ç»“æœä¸­çš„æ”¿åºœæ–‡ä»¶é“¾æ¥
- æ­¥éª¤3ï¼šæ»šåŠ¨é¡µé¢æŸ¥çœ‹æ›´å¤šå†…å®¹

**å½“å‰çŠ¶æ€**ï¼š
- å½“å‰é¡µé¢ï¼šhttps://www.gov.cn/article/xxx
- å‘ç°çš„å…³é”®ä¿¡æ¯ï¼š..."""
                    },
                    {
                        "role": "user",
                        "content": f"è¯·æ€»ç»“ä»¥ä¸‹å¯¹è¯å†å²ï¼š\n\n{history_text}"
                    }
                ],
                max_completion_tokens=500,  # GPT-5ä½¿ç”¨max_completion_tokensè€Œä¸æ˜¯max_tokens
                temperature=0.3
            )

            summary = response.choices[0].message.content.strip()
            logger.info(f"å†å²æ€»ç»“å®Œæˆï¼Œé•¿åº¦: {len(summary)} å­—ç¬¦")

            return summary

        except Exception as e:
            logger.error(f"æ€»ç»“å¯¹è¯å†å²å¤±è´¥: {e}")
            # è¿”å›ç®€å•çš„fallbackæ€»ç»“
            return f"[å¯¹è¯å†å²æ€»ç»“å¤±è´¥] å…± {len(self.conversation_history)} æ¡æ¶ˆæ¯"

    def _check_repeated_action(self, action: str, element_id: Optional[int], text: Optional[str]) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦æ˜¯é‡å¤çš„æ“ä½œ

        Args:
            action: æ“ä½œç±»å‹
            element_id: å…ƒç´ ID
            text: è¾“å…¥æ–‡æœ¬

        Returns:
            True å¦‚æœæ˜¯é‡å¤æ“ä½œ
        """
        action_tuple = (action, element_id, text)

        # æ£€æŸ¥æœ€è¿‘çš„æ“ä½œä¸­æ˜¯å¦æœ‰å®Œå…¨ç›¸åŒçš„
        repeated_count = self.recent_actions.count(action_tuple)

        return repeated_count >= 2  # å¦‚æœåŒæ ·çš„æ“ä½œå‡ºç°2æ¬¡æˆ–ä»¥ä¸Šï¼Œè®¤ä¸ºæ˜¯é‡å¤

    def _record_action(self, action: str, element_id: Optional[int], text: Optional[str]):
        """
        è®°å½•æ“ä½œåˆ°æœ€è¿‘æ“ä½œåˆ—è¡¨

        Args:
            action: æ“ä½œç±»å‹
            element_id: å…ƒç´ ID
            text: è¾“å…¥æ–‡æœ¬
        """
        action_tuple = (action, element_id, text)
        self.recent_actions.append(action_tuple)

        # åªä¿ç•™æœ€è¿‘çš„æ“ä½œ
        if len(self.recent_actions) > self.max_recent_actions:
            self.recent_actions = self.recent_actions[-self.max_recent_actions:]

    async def _compress_conversation_history(self):
        """
        å‹ç¼©å¯¹è¯å†å²ï¼ˆå½“æ¶ˆæ¯æ•°è¶…è¿‡é˜ˆå€¼æ—¶ï¼‰

        æµç¨‹ï¼š
        1. è°ƒç”¨ LLM æ€»ç»“æ—§çš„å¯¹è¯å†å²
        2. æ¸…ç†æ—§å†å²ï¼Œåªä¿ç•™æœ€è¿‘çš„å‡ è½®
        3. å°†æ€»ç»“ä½œä¸ºä¸Šä¸‹æ–‡æ’å…¥
        """
        if len(self.conversation_history) <= self.max_history_messages:
            return  # æœªè¶…è¿‡é˜ˆå€¼ï¼Œæ— éœ€å‹ç¼©

        logger.info(f"å¯¹è¯å†å²å·²è¾¾åˆ° {len(self.conversation_history)} æ¡ï¼Œå¼€å§‹å‹ç¼©...")

        # 1. æ€»ç»“æ—§çš„å¯¹è¯å†å²ï¼ˆé™¤äº†æœ€è¿‘5è½®ï¼‰
        old_history = self.conversation_history[:-10]  # ä¿ç•™æœ€è¿‘10æ¡æ¶ˆæ¯ä¸æ€»ç»“
        current_history = self.conversation_history

        # ä¸´æ—¶ä¿å­˜å½“å‰å†å²ï¼Œç”¨äºæ€»ç»“
        self.conversation_history = old_history
        summary = await self._summarize_conversation_history()
        self.conversation_history = current_history

        # 2. æ›´æ–°å†å²æ€»ç»“
        if self.history_summary:
            # å¦‚æœå·²æœ‰æ€»ç»“ï¼Œè¿½åŠ æ–°çš„æ€»ç»“
            self.history_summary += f"\n\n**åç»­æ“ä½œæ€»ç»“**ï¼š\n{summary}"
        else:
            self.history_summary = summary

        # 3. å‹ç¼©å†å²ï¼šåªä¿ç•™æœ€è¿‘10æ¡æ¶ˆæ¯
        self.conversation_history = self.conversation_history[-10:]

        logger.info(f"å‹ç¼©å®Œæˆï¼Œä¿ç•™äº†æœ€è¿‘ {len(self.conversation_history)} æ¡æ¶ˆæ¯")
        logger.info(f"å†å²æ€»ç»“é•¿åº¦: {len(self.history_summary)} å­—ç¬¦")

    def get_conversation_stats(self) -> Dict:
        """
        è·å–å¯¹è¯å†å²ç»Ÿè®¡ä¿¡æ¯

        Returns:
            {
                'total_messages': int,
                'user_messages': int,
                'assistant_messages': int,
                'estimated_tokens': int  # ç²—ç•¥ä¼°ç®—
            }
        """
        user_msgs = sum(1 for msg in self.conversation_history if msg['role'] == 'user')
        assistant_msgs = sum(1 for msg in self.conversation_history if msg['role'] == 'assistant')

        # ç²—ç•¥ä¼°ç®— tokensï¼ˆå‡è®¾æ¯ä¸ªå­—ç¬¦çº¦ 0.5 tokenï¼‰
        total_chars = 0
        for msg in self.conversation_history:
            if isinstance(msg['content'], str):
                total_chars += len(msg['content'])
            elif isinstance(msg['content'], list):
                for item in msg['content']:
                    if isinstance(item, dict) and item.get('type') == 'input_text':
                        total_chars += len(item.get('text', ''))

        # åŠ ä¸Š static system prompt çš„å­—ç¬¦æ•°
        total_chars += len(self.static_system_prompt) if self.static_system_prompt else 0

        return {
            'total_messages': len(self.conversation_history),
            'user_messages': user_msgs,
            'assistant_messages': assistant_msgs,
            'estimated_tokens': int(total_chars * 0.5)
        }

    def _export_site_graph(self) -> str:
        """
        å¯¼å‡ºé¡µé¢å›¾è°±ä¸º Markdown æŠ¥å‘Š

        Returns:
            Markdown æ ¼å¼çš„é¡µé¢å›¾è°±æŠ¥å‘Š
        """
        output = "# é¡µé¢å¯¼èˆªå›¾è°±\n\n"
        output += f"**æ€»è®¿é—®é¡µé¢æ•°**: {len(self.site_graph.nodes)}\n\n"

        # æŒ‰é¡µé¢ç±»å‹åˆ†ç»„
        pages_by_type = {
            "entry": [],
            "list": [],
            "detail": [],
            "other": []
        }

        for url, node in self.site_graph.nodes.items():
            pages_by_type[node.page_type].append(node)

        # è¾“å‡ºå„ç±»å‹é¡µé¢
        type_names = {
            "entry": "å…¥å£é¡µé¢",
            "list": "åˆ—è¡¨é¡µé¢",
            "detail": "è¯¦æƒ…é¡µé¢",
            "other": "å…¶ä»–é¡µé¢"
        }

        for page_type, type_name in type_names.items():
            pages = pages_by_type[page_type]
            if pages:
                output += f"## {type_name} ({len(pages)})\n\n"
                for node in pages:
                    output += f"### {node.title or 'æ— æ ‡é¢˜'}\n\n"
                    output += f"- **URL**: {node.url}\n"
                    output += f"- **è®¿é—®æ¬¡æ•°**: {node.visited_count}\n"
                    if node.description:
                        output += f"- **æè¿°**: {node.description}\n"
                    if node.parent_url:
                        parent = self.site_graph.nodes.get(node.parent_url)
                        if parent:
                            output += f"- **æ¥æº**: {parent.title or parent.url[:40]}\n"
                    output += "\n"

        # è¾“å‡ºå¯¼èˆªå†å²
        output += "## å¯¼èˆªå†å²\n\n"
        output += "```\n"
        for i, url in enumerate(self.site_graph.navigation_history, 1):
            node = self.site_graph.nodes.get(url)
            if node:
                output += f"{i}. [{node.page_type}] {node.title or url[:60]}\n"
        output += "```\n"

        return output

    def export_conversation_history(self, output_path: Optional[str] = None) -> str:
        """
        å¯¼å‡ºå¯¹è¯å†å²åˆ° JSON æ–‡ä»¶ï¼ˆç”¨äºè°ƒè¯•å’Œåˆ†æï¼‰

        Args:
            output_path: è¾“å‡ºè·¯å¾„ï¼ˆå¯é€‰ï¼‰

        Returns:
            å¯¼å‡ºçš„æ–‡ä»¶è·¯å¾„
        """
        if not output_path:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = self.reports_dir / f"conversation_history_{timestamp}.json"
        else:
            output_path = Path(output_path)

        export_data = {
            'query': self.current_query,
            'static_system_prompt': self.static_system_prompt,
            'conversation_history': self.conversation_history,
            'stats': self.get_conversation_stats()
        }

        output_path.write_text(json.dumps(export_data, ensure_ascii=False, indent=2), encoding='utf-8')
        logger.info(f"å¯¹è¯å†å²å·²å¯¼å‡ºåˆ°: {output_path}")

        return str(output_path)

    async def execute_task(self, query: str, target_url: str, task_id: int) -> Dict:
        """
        æ‰§è¡Œæ·±åº¦æœç´¢ä»»åŠ¡

        Args:
            query: ç”¨æˆ·é—®é¢˜
            target_url: ç›®æ ‡ç½‘ç«™URL
            task_id: ä»»åŠ¡ID

        Returns:
            {
                'success': bool,
                'summary': str,
                'source_url': str,
                'citations': List[str],
                'steps': List[Dict],
                'report_html_path': str,
                'error': str (if failed)
            }
        """
        logger.info(f"å¼€å§‹æ‰§è¡Œä»»åŠ¡ {task_id}: {query} -> {target_url}")

        async with async_playwright() as p:
            # å¯åŠ¨æµè§ˆå™¨ï¼ˆæ”¯æŒæœ‰å¤´æ¨¡å¼è§‚å¯Ÿæ“ä½œè¿‡ç¨‹ï¼‰
            # æ·»åŠ å¼ºåŒ–çš„åæ£€æµ‹å‚æ•°
            browser_args = [
                '--disable-blink-features=AutomationControlled',
                '--disable-dev-shm-usage',
                '--disable-web-security',
                '--disable-features=IsolateOrigins,site-per-process',
                '--disable-site-isolation-trials',
            ]

            if not self.headless:
                browser_args.append('--start-maximized')

            # å¯¹äºé«˜é˜²æŠ¤ç½‘ç«™ï¼ˆå¦‚tesla.comï¼‰ï¼Œä½¿ç”¨çœŸå®Chromeæµè§ˆå™¨
            use_real_chrome = any(domain in target_url for domain in ['tesla.com', 'saytechnologies.com'])

            if use_real_chrome and not self.headless:
                # æ–¹æ¡ˆï¼šä½¿ç”¨ç³»ç»Ÿå·²å®‰è£…çš„çœŸå®Chromeï¼ˆæœ€éš¾è¢«æ£€æµ‹ï¼‰
                logger.info("æ£€æµ‹åˆ°é«˜é˜²æŠ¤ç½‘ç«™ï¼Œä½¿ç”¨çœŸå®Chromeæµè§ˆå™¨...")

                # ä½¿ç”¨channelå‚æ•°è¿æ¥ç³»ç»ŸChrome
                try:
                    browser = await p.chromium.launch(
                        channel='chrome',  # ä½¿ç”¨ç³»ç»Ÿå®‰è£…çš„Chrome
                        headless=False,
                        slow_mo=self.slow_mo,
                        args=['--start-maximized']
                    )
                except Exception as e:
                    logger.warning(f"æ— æ³•å¯åŠ¨ç³»ç»ŸChrome: {e}ï¼Œå›é€€åˆ°Chromium")
                    browser = await p.chromium.launch(
                        headless=self.headless,
                        slow_mo=self.slow_mo,
                        args=browser_args
                    )
            else:
                # å…¶ä»–æƒ…å†µä½¿ç”¨æ ‡å‡†Chromium
                browser = await p.chromium.launch(
                    headless=self.headless,
                    slow_mo=self.slow_mo,
                    args=browser_args
                )

            # åˆ›å»ºä¸‹è½½ç›®å½•
            download_path = os.path.join(os.getcwd(), "downloads")
            os.makedirs(download_path, exist_ok=True)
            logger.info(f"ğŸ“ ä¸‹è½½è·¯å¾„: {download_path}")

            # åˆ›å»ºæµè§ˆå™¨ä¸Šä¸‹æ–‡ï¼ˆé…ç½®ä¸‹è½½è·¯å¾„ï¼‰
            context = await browser.new_context(
                viewport={'width': 1280, 'height': 1024},
                accept_downloads=True  # å…è®¸ä¸‹è½½
            )

            # åˆ›å»ºé¡µé¢å¹¶è®¾ç½®åæ£€æµ‹
            page = await context.new_page()

            # è®¾ç½®çœŸå®çš„User-Agentå’Œheaders
            await page.set_extra_http_headers({
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'Sec-Ch-Ua': '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
                'Sec-Ch-Ua-Mobile': '?0',
                'Sec-Ch-Ua-Platform': '"macOS"',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Sec-Fetch-User': '?1',
                'Upgrade-Insecure-Requests': '1'
            })

            # æ³¨å…¥å¼ºåŒ–çš„åæ£€æµ‹è„šæœ¬
            await page.add_init_script("""
                // è¦†ç›–webdriverå±æ€§
                Object.defineProperty(navigator, 'webdriver', {
                    get: () => undefined
                });

                // è¦†ç›–permissions
                const originalQuery = window.navigator.permissions.query;
                window.navigator.permissions.query = (parameters) => (
                    parameters.name === 'notifications' ?
                        Promise.resolve({ state: Notification.permission }) :
                        originalQuery(parameters)
                );

                // æ·»åŠ å®Œæ•´çš„chromeå¯¹è±¡
                window.chrome = {
                    runtime: {},
                    loadTimes: function() {},
                    csi: function() {},
                    app: {}
                };

                // è¦†ç›–plugins
                Object.defineProperty(navigator, 'plugins', {
                    get: () => [1, 2, 3, 4, 5]
                });

                // è¦†ç›–languages
                Object.defineProperty(navigator, 'languages', {
                    get: () => ['zh-CN', 'zh', 'en']
                });

                // è¦†ç›–platform
                Object.defineProperty(navigator, 'platform', {
                    get: () => 'MacIntel'
                });

                // æ¨¡æ‹ŸçœŸå®çš„ç¡¬ä»¶å¹¶å‘æ•°
                Object.defineProperty(navigator, 'hardwareConcurrency', {
                    get: () => 8
                });

                // è¦†ç›–deviceMemory
                Object.defineProperty(navigator, 'deviceMemory', {
                    get: () => 8
                });

                // æ·»åŠ battery API
                navigator.getBattery = () => {
                    return Promise.resolve({
                        charging: true,
                        chargingTime: 0,
                        dischargingTime: Infinity,
                        level: 1
                    });
                };
            """)

            try:
                # è·å–browser contextï¼ˆç”¨äºæ‰¹é‡æ‰§è¡Œæ—¶åˆ›å»ºæ–°æ ‡ç­¾é¡µï¼‰
                context = page.context

                # æ‰§è¡Œä»»åŠ¡
                result = await self._run_browser_loop(
                    page=page,
                    context=context,
                    query=query,
                    target_url=target_url,
                    task_id=task_id
                )

                return result

            except Exception as e:
                logger.error(f"ä»»åŠ¡ {task_id} æ‰§è¡Œå¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                return {
                    'success': False,
                    'error': str(e)
                }
            finally:
                await browser.close()

    async def _get_interactive_elements(self, page: Page) -> List[dict]:
        """
        æå–é¡µé¢ä¸­æ‰€æœ‰å¯äº¤äº’å…ƒç´ 

        Returns:
            å…ƒç´ åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«: id, type, text, selector, bbox
        """
        elements_data = await page.evaluate(r"""
            () => {
                const elements = [];
                let id = 1;

                // ä¼˜åŒ–çš„äº¤äº’å…ƒç´ é€‰æ‹©å™¨ï¼ˆæ›´å…¨é¢ï¼‰
                const interactiveSelectors = [
                    'input:not([type="hidden"])',
                    'textarea',
                    'button',
                    'a',  // æ‰€æœ‰aæ ‡ç­¾ï¼ˆä¸ç®¡æ˜¯å¦æœ‰hrefï¼‰
                    'select',
                    '[role="button"]',
                    '[role="link"]',
                    '[role="textbox"]',
                    '[onclick]',
                    '[ng-click]',  // Angularç‚¹å‡»äº‹ä»¶
                    '[data-click]',  // è‡ªå®šä¹‰ç‚¹å‡»å±æ€§
                    '[data-link]',  // è‡ªå®šä¹‰é“¾æ¥å±æ€§
                    '[contenteditable="true"]',
                    // å¸¸è§çš„å¯ç‚¹å‡»å®¹å™¨
                    'div[onclick]',
                    'span[onclick]',
                    'li[onclick]',
                    'td[onclick]',
                    'tr[onclick]'
                ];

                // é¦–å…ˆè·å–åŸºæœ¬çš„å¯äº¤äº’å…ƒç´ 
                let allElements = Array.from(document.querySelectorAll(
                    interactiveSelectors.join(',')
                ));

                // é¢å¤–æŸ¥æ‰¾ï¼šæ‰€æœ‰cursorä¸ºpointerçš„å…ƒç´ ï¼ˆé€šå¸¸å¯ç‚¹å‡»ï¼‰
                const allDivSpans = document.querySelectorAll('div, span, td, tr, li, p');
                allDivSpans.forEach(el => {
                    const style = window.getComputedStyle(el);
                    if (style.cursor === 'pointer' && !allElements.includes(el)) {
                        allElements.push(el);
                    }
                });

                allElements.forEach(el => {
                    // è·å–å…ƒç´ ä½ç½®
                    const rect = el.getBoundingClientRect();

                    const style = window.getComputedStyle(el);

                    // è¿‡æ»¤çœŸæ­£ä¸å¯è§çš„å…ƒç´ 
                    if (style.display === 'none') return;
                    if (style.visibility === 'hidden') return;
                    if (parseFloat(style.opacity) < 0.1) return;

                    // å¯¹äºé“¾æ¥å…ƒç´ ï¼Œå³ä½¿å°ºå¯¸ä¸º0ä¹Ÿå¯èƒ½æ˜¯æœ‰æ•ˆçš„ï¼ˆè¡¨æ ¼å•å…ƒæ ¼ä¸­çš„é“¾æ¥ï¼‰
                    // åªè¦å®ƒæœ‰æ–‡æœ¬å†…å®¹å°±ä¿ç•™
                    const isLink = el.tagName === 'A';
                    const hasText = el.innerText && el.innerText.trim().length > 0;

                    // è¿‡æ»¤é›¶å°ºå¯¸å…ƒç´ ï¼ˆä½†ä¿ç•™æœ‰æ–‡æœ¬çš„é“¾æ¥ï¼‰
                    if (rect.width === 0 || rect.height === 0) {
                        if (!isLink || !hasText) return;
                    }

                    // æ”¾å®½è§†å£è¿‡æ»¤ï¼šå…è®¸æå–è§†å£ä¸‹æ–¹2000pxå†…çš„å…ƒç´ ï¼ˆé€‚ç”¨äºè¡¨æ ¼ã€åˆ—è¡¨ç­‰ï¼‰
                    if (rect.top < -500 || rect.top > window.innerHeight + 2000) return;

                    // ç”Ÿæˆç¨³å®šçš„CSS selectorï¼ˆä¼˜å…ˆçº§ç­–ç•¥ï¼‰
                    let selector = '';

                    // ä¼˜å…ˆçº§1: IDï¼ˆæœ€ç¨³å®šï¼‰
                    if (el.id) {
                        selector = `#${el.id}`;
                    }
                    // ä¼˜å…ˆçº§2: nameå±æ€§
                    else if (el.name) {
                        selector = `${el.tagName.toLowerCase()}[name="${el.name}"]`;
                    }
                    // ä¼˜å…ˆçº§3: data-*å±æ€§
                    else if (el.dataset && Object.keys(el.dataset).length > 0) {
                        const key = Object.keys(el.dataset)[0];
                        const value = el.dataset[key];
                        selector = `[data-${key}="${value}"]`;
                    }
                    // ä¼˜å…ˆçº§4: classï¼ˆé€‰æ‹©æœ€å…·ä½“çš„classï¼‰
                    else if (el.className && typeof el.className === 'string') {
                        const classes = el.className.trim().split(/\\s+/)
                            .filter(c => c.length > 0 && c.length < 30)  // è¿‡æ»¤å¤ªé•¿çš„class
                            .slice(0, 2);  // æœ€å¤šç”¨2ä¸ªclass
                        if (classes.length > 0) {
                            selector = `${el.tagName.toLowerCase()}.${classes.join('.')}`;
                        }
                    }
                    // ä¼˜å…ˆçº§5: nth-of-typeï¼ˆæœ€åæ‰‹æ®µï¼‰
                    if (!selector) {
                        const parent = el.parentElement;
                        if (parent) {
                            const siblings = Array.from(parent.children)
                                .filter(e => e.tagName === el.tagName);
                            const index = siblings.indexOf(el) + 1;
                            selector = `${el.tagName.toLowerCase()}:nth-of-type(${index})`;
                        } else {
                            selector = el.tagName.toLowerCase();
                        }
                    }

                    // ä¼˜åŒ–çš„æ–‡æœ¬æå–ï¼ˆæ”¯æŒæ›´é•¿çš„æ ‡é¢˜ï¼‰
                    let text = '';

                    // ä¼˜å…ˆä½¿ç”¨innerTextï¼ˆæ”¯æŒæ¢è¡Œå’Œå¯è§æ–‡æœ¬ï¼‰
                    if (el.innerText && el.innerText.trim()) {
                        let rawText = el.innerText.trim();
                        // æ¸…ç†å¤šä½™çš„ç©ºç™½å­—ç¬¦å’Œæ¢è¡Œ
                        rawText = rawText.replace(/\s+/g, ' ');
                        // å¯¹äºé“¾æ¥å’Œæ ‡é¢˜ï¼Œä¿ç•™æ›´å¤šæ–‡æœ¬ï¼ˆæœ€å¤š100å­—ç¬¦ï¼‰
                        const maxLength = (el.tagName === 'A' || el.tagName === 'H1' || el.tagName === 'H2' || el.tagName === 'H3') ? 100 : 50;
                        text = rawText.substring(0, maxLength);
                    }
                    // textContentä½œä¸ºå¤‡é€‰ï¼ˆåŒ…å«éšè—æ–‡æœ¬ï¼‰
                    else if (el.textContent && el.textContent.trim()) {
                        let rawText = el.textContent.trim().replace(/\s+/g, ' ');
                        text = rawText.substring(0, 50);
                    }
                    // å…¶ä»–å±æ€§
                    else if (el.placeholder) {
                        text = `[placeholder: ${el.placeholder.substring(0, 50)}]`;
                    } else if (el.getAttribute('aria-label')) {
                        text = `[aria: ${el.getAttribute('aria-label').substring(0, 50)}]`;
                    } else if (el.value) {
                        text = `[value: ${el.value.substring(0, 30)}]`;
                    } else if (el.alt) {
                        text = `[alt: ${el.alt.substring(0, 50)}]`;
                    } else if (el.title) {
                        text = `[title: ${el.title.substring(0, 50)}]`;
                    }

                    // è·³è¿‡ç©ºæ–‡æœ¬å…ƒç´ ï¼ˆä½†ä¿ç•™è¾“å…¥æ¡†ï¼‰
                    if (!text && el.tagName !== 'INPUT' && el.tagName !== 'TEXTAREA' && el.tagName !== 'SELECT') {
                        return;
                    }

                    // æ£€æµ‹æ˜¯å¦ä¸ºå¯æ»šåŠ¨å®¹å™¨
                    let isScrollable = false;
                    const overflowY = style.overflowY;
                    const overflowX = style.overflowX;

                    if ((overflowY === 'scroll' || overflowY === 'auto' ||
                         overflowX === 'scroll' || overflowX === 'auto') &&
                        (el.scrollHeight > el.clientHeight || el.scrollWidth > el.clientWidth)) {
                        isScrollable = true;
                    }

                    // ä¸ºæ¯ä¸ªå…ƒç´ æ·»åŠ å”¯ä¸€çš„dataå±æ€§ï¼ˆç¡®ä¿ä¸€ä¸€å¯¹åº”ï¼‰
                    const uniqueId = `ba-${id}`;
                    el.setAttribute('data-browser-agent-id', uniqueId);

                    // å¯¹äºé›¶å°ºå¯¸çš„é“¾æ¥ï¼Œå°è¯•ä½¿ç”¨çˆ¶å…ƒç´ çš„å°ºå¯¸
                    let finalRect = rect;
                    if ((rect.width === 0 || rect.height === 0) && isLink && el.parentElement) {
                        finalRect = el.parentElement.getBoundingClientRect();
                    }

                    elements.push({
                        id: id++,
                        type: el.tagName.toLowerCase(),
                        role: el.getAttribute('role') || '',
                        text: text,
                        selector: selector,
                        uniqueSelector: `[data-browser-agent-id="${uniqueId}"]`,  // å”¯ä¸€selector
                        isScrollable: isScrollable,  // æ ‡è®°æ˜¯å¦å¯æ»šåŠ¨
                        bbox: {
                            x: Math.round(finalRect.x),
                            y: Math.round(finalRect.y),
                            width: Math.round(finalRect.width),
                            height: Math.round(finalRect.height)
                        }
                    });
                });

                // é¢å¤–æ£€æµ‹iframeå’Œå¯æ»šåŠ¨å®¹å™¨ï¼ˆå³ä½¿å®ƒä»¬ä¸æ˜¯å¯äº¤äº’å…ƒç´ ï¼‰
                const scrollableContainers = document.querySelectorAll('iframe, [style*="overflow"]');
                scrollableContainers.forEach(el => {
                    // å¦‚æœå·²ç»è¢«æ·»åŠ ï¼Œè·³è¿‡
                    if (el.hasAttribute('data-browser-agent-id')) return;

                    const rect = el.getBoundingClientRect();
                    if (rect.width === 0 || rect.height === 0) return;
                    if (rect.top < -500 || rect.top > window.innerHeight + 2000) return;

                    const style = window.getComputedStyle(el);
                    if (style.display === 'none' || style.visibility === 'hidden') return;

                    // æ£€æŸ¥æ˜¯å¦çœŸçš„å¯æ»šåŠ¨
                    const overflowY = style.overflowY;
                    const overflowX = style.overflowX;
                    const isScrollable = (overflowY === 'scroll' || overflowY === 'auto' ||
                                         overflowX === 'scroll' || overflowX === 'auto') &&
                                        (el.scrollHeight > el.clientHeight || el.scrollWidth > el.clientWidth);

                    const isIframe = el.tagName.toLowerCase() === 'iframe';

                    if (!isScrollable && !isIframe) return;

                    const uniqueId = `ba-${id}`;
                    el.setAttribute('data-browser-agent-id', uniqueId);

                    let text = '[å¯æ»šåŠ¨å®¹å™¨]';
                    if (isIframe) {
                        text = '[iframe]';
                        const title = el.getAttribute('title');
                        if (title) text = `[iframe: ${title.substring(0, 30)}]`;
                    }

                    elements.push({
                        id: id++,
                        type: el.tagName.toLowerCase(),
                        role: '',
                        text: text,
                        selector: el.id ? `#${el.id}` : el.tagName.toLowerCase(),
                        uniqueSelector: `[data-browser-agent-id="${uniqueId}"]`,
                        isScrollable: true,
                        bbox: {
                            x: Math.round(rect.x),
                            y: Math.round(rect.y),
                            width: Math.round(rect.width),
                            height: Math.round(rect.height)
                        }
                    });
                });

                return elements;
            }
        """)

        logger.info(f"æå–åˆ° {len(elements_data)} ä¸ªå¯äº¤äº’å…ƒç´ ")
        return elements_data

    async def _get_or_cache_elements(self, page: Page, force_refresh: bool = False) -> List[dict]:
        """
        è·å–é¡µé¢å…ƒç´ ï¼ˆå¸¦ç¼“å­˜ï¼‰

        Args:
            page: å½“å‰é¡µé¢
            force_refresh: å¼ºåˆ¶åˆ·æ–°ï¼Œä¸ä½¿ç”¨ç¼“å­˜

        Returns:
            å…ƒç´ åˆ—è¡¨
        """
        import hashlib

        current_url = page.url

        # è®¡ç®—é¡µé¢HTMLçš„å“ˆå¸Œï¼ˆç”¨äºæ£€æµ‹å˜åŒ–ï¼‰
        html_hash = await page.evaluate("() => document.body.innerHTML.substring(0, 2000)")
        html_hash = hashlib.md5(html_hash.encode()).hexdigest()

        # æ£€æŸ¥ç¼“å­˜
        if not force_refresh and current_url in self.page_cache:
            snapshot = self.page_cache[current_url]
            if snapshot.is_valid(current_url, html_hash):
                logger.info(f"âœ… ä½¿ç”¨ç¼“å­˜çš„é¡µé¢å…ƒç´  ({len(snapshot.elements)}ä¸ª)")
                return snapshot.elements
            else:
                logger.info("âš ï¸  é¡µé¢å·²å˜åŒ–ï¼Œé‡æ–°æå–å…ƒç´ ")

        # æå–å…ƒç´ 
        logger.info("ğŸ” æå–é¡µé¢å…ƒç´ ...")
        elements = await self._get_interactive_elements(page)

        # ç¼“å­˜
        self.page_cache[current_url] = PageSnapshot(
            url=current_url,
            timestamp=datetime.now(),
            elements=elements,
            html_hash=html_hash
        )

        logger.info(f"ğŸ’¾ å·²ç¼“å­˜é¡µé¢å…ƒç´  ({len(elements)}ä¸ª)")
        return elements

    def _setup_download_listener(self, page):
        """
        ä¸ºæŒ‡å®šçš„pageå¯¹è±¡è®¾ç½®ä¸‹è½½äº‹ä»¶ç›‘å¬å™¨

        Args:
            page: Playwright Pageå¯¹è±¡
        """
        async def handle_download(download):
            """å¤„ç†ä¸‹è½½äº‹ä»¶"""
            try:
                # è·å–å»ºè®®çš„æ–‡ä»¶å
                suggested_filename = download.suggested_filename
                # ä¿å­˜åˆ°downloadsç›®å½•
                file_path = os.path.join(self.download_path, suggested_filename)

                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ï¼ˆå¿½ç•¥å¤§å°å†™ï¼Œé¿å…é‡å¤ä¸‹è½½ï¼‰
                existing_files = [f.lower() for f in self.downloaded_files]
                if file_path.lower() in existing_files:
                    logger.info(f"â­ï¸  æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½: {suggested_filename}")
                    return

                await download.save_as(file_path)
                self.downloaded_files.append(file_path)
                logger.info(f"âœ… æ–‡ä»¶å·²ä¸‹è½½: {file_path}")
            except Exception as e:
                logger.error(f"âŒ ä¸‹è½½å¤±è´¥: {e}")

        # ç›‘å¬ä¸‹è½½äº‹ä»¶
        page.on("download", handle_download)
        logger.info(f"ğŸ“¥ å·²ä¸ºé¡µé¢è®¾ç½®ä¸‹è½½ç›‘å¬å™¨: {page.url[:60]}...")

    def _get_downloads_info(self) -> str:
        """
        è·å–ä¸‹è½½ç›®å½•ä¸­çš„æ–‡ä»¶ä¿¡æ¯

        Returns:
            str: æ ¼å¼åŒ–çš„æ–‡ä»¶åˆ—è¡¨ä¿¡æ¯
        """
        download_path = os.path.join(os.getcwd(), "downloads")

        if not os.path.exists(download_path):
            return "ğŸ“¥ **ä¸‹è½½ç›®å½•**: ç©ºï¼ˆå°šæœªä¸‹è½½ä»»ä½•æ–‡ä»¶ï¼‰"

        files = []
        for filename in os.listdir(download_path):
            file_path = os.path.join(download_path, filename)
            if os.path.isfile(file_path):
                # è·å–æ–‡ä»¶ä¿¡æ¯
                size = os.path.getsize(file_path)
                size_kb = size / 1024
                mtime = os.path.getmtime(file_path)
                from datetime import datetime
                mod_time = datetime.fromtimestamp(mtime).strftime('%Y-%m-%d %H:%M:%S')

                files.append({
                    'name': filename,
                    'size_kb': size_kb,
                    'modified': mod_time
                })

        if not files:
            return "ğŸ“¥ **ä¸‹è½½ç›®å½•**: ç©ºï¼ˆå°šæœªä¸‹è½½ä»»ä½•æ–‡ä»¶ï¼‰"

        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
        files.sort(key=lambda x: x['modified'], reverse=True)

        # æ ¼å¼åŒ–è¾“å‡º
        info_lines = [f"ğŸ“¥ **ä¸‹è½½ç›®å½•**: {len(files)} ä¸ªæ–‡ä»¶"]
        for i, f in enumerate(files, 1):
            info_lines.append(f"  {i}. {f['name']} ({f['size_kb']:.1f}KB, {f['modified']})")

        return "\n".join(info_lines)

    def _annotate_screenshot(self, screenshot_bytes: bytes, elements: List[dict]) -> bytes:
        """
        åœ¨æˆªå›¾ä¸Šç»˜åˆ¶å…ƒç´ æ ‡æ³¨ï¼ˆçº¢æ¡†+ç¼–å·ï¼‰

        Args:
            screenshot_bytes: åŸå§‹æˆªå›¾å­—èŠ‚
            elements: å…ƒç´ åˆ—è¡¨

        Returns:
            æ ‡æ³¨åçš„æˆªå›¾å­—èŠ‚
        """
        if not Image or not ImageDraw or not ImageFont:
            logger.warning("PILæœªå®‰è£…ï¼Œè¿”å›åŸå§‹æˆªå›¾")
            return screenshot_bytes

        try:
            # åŠ è½½æˆªå›¾
            image = Image.open(io.BytesIO(screenshot_bytes))
            draw = ImageDraw.Draw(image)

            # å°è¯•åŠ è½½å­—ä½“
            font = None
            font_paths = [
                "/System/Library/Fonts/Helvetica.ttc",  # macOS
                "/System/Library/Fonts/Supplemental/Arial.ttf",  # macOS
                "C:\\Windows\\Fonts\\arial.ttf",  # Windows
                "/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf",  # Linux
            ]
            for font_path in font_paths:
                try:
                    font = ImageFont.truetype(font_path, 16)
                    break
                except:
                    continue

            if not font:
                font = ImageFont.load_default()

            # è·å–æˆªå›¾å°ºå¯¸
            img_width, img_height = image.size

            # åªæ ‡æ³¨åœ¨æˆªå›¾èŒƒå›´å†…å¯è§çš„å…ƒç´ ï¼ˆé¿å…æ ‡æ³¨è§†å£å¤–çš„å…ƒç´ ï¼‰
            visible_elements = []
            for elem in elements:
                bbox = elem['bbox']
                x, y, w, h = bbox['x'], bbox['y'], bbox['width'], bbox['height']

                # æ£€æŸ¥å…ƒç´ æ˜¯å¦åœ¨æˆªå›¾èŒƒå›´å†…ï¼ˆè‡³å°‘éƒ¨åˆ†å¯è§ï¼‰
                if x < img_width and y < img_height and (x + w) > 0 and (y + h) > 0:
                    visible_elements.append(elem)

            logger.info(f"æˆªå›¾å°ºå¯¸: {img_width}x{img_height}, å¯è§å…ƒç´ : {len(visible_elements)}/{len(elements)}")

            # ç»˜åˆ¶æ¯ä¸ªå¯è§å…ƒç´ çš„æ ‡æ³¨ï¼ˆé™åˆ¶æœ€å¤š100ä¸ªé¿å…å¤ªä¹±ï¼‰
            for elem in visible_elements[:100]:
                bbox = elem['bbox']
                x, y, w, h = bbox['x'], bbox['y'], bbox['width'], bbox['height']

                # ç»˜åˆ¶çº¢è‰²è¾¹æ¡†ï¼ˆ2pxï¼‰
                draw.rectangle(
                    [(x, y), (x + w, y + h)],
                    outline='#FF0000',
                    width=2
                )

                # ç»˜åˆ¶ç¼–å·æ ‡ç­¾
                label = f"[{elem['id']}]"

                # è®¡ç®—æ ‡ç­¾å°ºå¯¸
                try:
                    bbox_text = draw.textbbox((0, 0), label, font=font)
                    label_width = bbox_text[2] - bbox_text[0] + 8
                    label_height = bbox_text[3] - bbox_text[1] + 4
                except:
                    label_width = len(label) * 10
                    label_height = 18

                # æ ‡ç­¾ä½ç½®ï¼ˆé¿å…è¶…å‡ºé¡¶éƒ¨ï¼‰
                label_y = max(0, y - label_height - 2)

                # æ ‡ç­¾èƒŒæ™¯ï¼ˆçº¢è‰²ï¼‰
                draw.rectangle(
                    [(x, label_y), (x + label_width, label_y + label_height)],
                    fill='#FF0000'
                )

                # æ ‡ç­¾æ–‡å­—ï¼ˆç™½è‰²ï¼‰
                draw.text(
                    (x + 4, label_y + 2),
                    label,
                    fill='white',
                    font=font
                )

            # è½¬æ¢å›bytes
            output = io.BytesIO()
            image.save(output, format='PNG')
            return output.getvalue()

        except Exception as e:
            logger.error(f"æˆªå›¾æ ‡æ³¨å¤±è´¥: {e}")
            return screenshot_bytes

    async def _run_browser_loop(self, page: Page, context, query: str, target_url: str, task_id: int) -> Dict:
        """
        æ‰§è¡Œæµè§ˆå™¨æ“ä½œå¾ªç¯

        Args:
            page: å½“å‰é¡µé¢
            context: Browser contextï¼ˆç”¨äºåˆ›å»ºæ–°æ ‡ç­¾é¡µï¼‰
            query: ç”¨æˆ·æŸ¥è¯¢
            target_url: ç›®æ ‡URL
            task_id: ä»»åŠ¡ID
        """
        steps = []
        step_count = 0

        # åˆå§‹åŒ–å¯¹è¯å†å²
        self._init_conversation(query)
        logger.info(f"å·²åˆå§‹åŒ–å¯¹è¯å†å²ï¼Œä»»åŠ¡: {query}")

        # ğŸ†• ä»»åŠ¡æ‹†è§£
        await self.task_manager.decompose_task(query)
        logger.info(f"ä»»åŠ¡å·²æ‹†è§£ä¸º {len(self.task_manager.subtasks)} ä¸ªå­ä»»åŠ¡")

        # ğŸ“¥ åˆå§‹åŒ–ä¸‹è½½ç›¸å…³å˜é‡
        self.download_path = os.path.join(os.getcwd(), "downloads")
        self.downloaded_files = []  # è®°å½•å·²ä¸‹è½½çš„æ–‡ä»¶ï¼ˆä»»åŠ¡çº§åˆ«ï¼‰

        # ğŸ“¥ è®¾ç½®ä¸‹è½½äº‹ä»¶ç›‘å¬ï¼ˆä¸ºåˆå§‹é¡µé¢ï¼‰
        self._setup_download_listener(page)
        logger.info(f"ğŸ“¥ ä¸‹è½½ç›‘å¬å·²å¯åŠ¨ï¼Œæ–‡ä»¶å°†ä¿å­˜åˆ°: {self.download_path}")

        # è®¿é—®ç›®æ ‡URL
        logger.info(f"è®¿é—®ç›®æ ‡URL: {target_url}")
        await page.goto(target_url, wait_until='networkidle', timeout=30000)
        current_url = page.url

        # ğŸ†• æ·»åŠ åˆå§‹é¡µé¢åˆ°å›¾è°±
        self.site_graph.add_or_update_page(
            url=current_url,
            page_type="entry",
            title="èµ·å§‹é¡µé¢",
            description="ä»»åŠ¡å¼€å§‹çš„é¡µé¢"
        )

        # ä¸»å¾ªç¯ï¼ˆè§†è§‰æ ‡æ³¨æ–¹æ¡ˆï¼‰
        elements = []  # å½“å‰é¡µé¢çš„å¯äº¤äº’å…ƒç´ åˆ—è¡¨

        while step_count < self.max_steps:
            step_count += 1
            logger.info(f"æ‰§è¡Œç¬¬ {step_count} æ­¥...")

            # âš ï¸ é‡è¦ï¼šå…ˆæ›´æ–° current_urlï¼Œç¡®ä¿æˆªå›¾å’Œ URL ä¸€è‡´
            current_url = page.url
            logger.info(f"ğŸ“ å½“å‰é¡µé¢: {current_url}")

            # 1. æå–å¯äº¤äº’å…ƒç´ 
            elements = await self._get_interactive_elements(page)

            # 2. æˆªå›¾ï¼ˆåŸå§‹ï¼‰
            screenshot_bytes = await page.screenshot(full_page=False)

            # 3. åœ¨æˆªå›¾ä¸Šæ ‡æ³¨å…ƒç´ 
            annotated_screenshot_bytes = self._annotate_screenshot(screenshot_bytes, elements)

            # 4. ä¿å­˜æ ‡æ³¨åçš„æˆªå›¾
            screenshot_path = self.screenshots_dir / f"task_{task_id}_step_{step_count}_annotated.png"
            screenshot_path.write_bytes(annotated_screenshot_bytes)
            logger.info(f"ğŸ“¸ æˆªå›¾å·²ä¿å­˜: {screenshot_path.name} (é¡µé¢: {current_url[:60]}...)")

            # 5. è½¬æ¢ä¸ºbase64
            screenshot_base64 = base64.b64encode(annotated_screenshot_bytes).decode()

            # 6. è°ƒç”¨è§†è§‰æ¨¡å‹åˆ†æï¼ˆå‘é€æ ‡æ³¨åçš„æˆªå›¾å’Œå…ƒç´ åˆ—è¡¨ï¼‰
            decision = await self._analyze_page(
                screenshot_base64=screenshot_base64,
                elements=elements,
                query=query,
                current_url=current_url,  # âœ… ç°åœ¨è¿™ä¸ª URL å’Œæˆªå›¾ä¸€è‡´
                step_count=step_count,
                task_id=task_id
            )

            # è®°å½•æ­¥éª¤
            step_record = {
                'step': step_count,
                'screenshot': str(screenshot_path),
                'url': current_url,
                'action': decision.get('action'),
                'reasoning': decision.get('reasoning', ''),
                'element_id': decision.get('element_id')
            }
            steps.append(step_record)

            logger.info(f"å†³ç­–: {decision.get('action')} - {decision.get('reasoning', '')[:100]}")

            # 7. æ£€æŸ¥æ˜¯å¦å®Œæˆ
            if decision.get('action') == 'TASK_COMPLETE':
                logger.info("ä»»åŠ¡å®Œæˆï¼")

                # è·å–æœ€ç»ˆé¡µé¢HTML
                html_content = await page.content()

                # ç”Ÿæˆé«˜äº®HTML
                report_html_path = await self._generate_highlighted_html(
                    html_content=html_content,
                    citations=decision.get('citations', []),
                    task_id=task_id
                )

                # è‡ªåŠ¨å¯¼å‡ºå¯¹è¯å†å²
                history_path = self.export_conversation_history()
                logger.info(f"å¯¹è¯å†å²å·²è‡ªåŠ¨å¯¼å‡º: {history_path}")

                # ğŸ†• å¯¼å‡ºä»»åŠ¡æŠ¥å‘Š
                task_report = self.task_manager.export_results()
                task_report_path = self.reports_dir / f"task_{task_id}_subtasks.md"
                task_report_path.write_text(task_report, encoding='utf-8')
                logger.info(f"ğŸ“‹ ä»»åŠ¡æŠ¥å‘Šå·²å¯¼å‡º: {task_report_path}")

                # ğŸ†• å¯¼å‡ºé¡µé¢å›¾è°±
                graph_report = self._export_site_graph()
                graph_report_path = self.reports_dir / f"task_{task_id}_site_graph.md"
                graph_report_path.write_text(graph_report, encoding='utf-8')
                logger.info(f"ğŸ—ºï¸  é¡µé¢å›¾è°±å·²å¯¼å‡º: {graph_report_path}")

                # ğŸ“¥ æ±‡æ€»ä¸‹è½½æ–‡ä»¶
                logger.info(f"ğŸ“¥ å…±ä¸‹è½½äº† {len(self.downloaded_files)} ä¸ªæ–‡ä»¶:")
                for file_path in self.downloaded_files:
                    logger.info(f"   - {os.path.basename(file_path)}")

                return {
                    'success': True,
                    'summary': decision.get('summary', ''),
                    'source_url': current_url,
                    'citations': decision.get('citations', []),
                    'steps': steps,
                    'report_html_path': report_html_path,
                    'conversation_history_path': history_path,  # è¿”å›å¯¹è¯å†å²è·¯å¾„
                    'task_report_path': str(task_report_path),  # ğŸ†• ä»»åŠ¡æŠ¥å‘Šè·¯å¾„
                    'graph_report_path': str(graph_report_path),  # ğŸ†• é¡µé¢å›¾è°±è·¯å¾„
                    'downloaded_files': self.downloaded_files,  # ğŸ“¥ ä¸‹è½½çš„æ–‡ä»¶åˆ—è¡¨
                    'download_count': len(self.downloaded_files)  # ğŸ“¥ ä¸‹è½½æ–‡ä»¶æ•°é‡
                }

            # 8. æ‰§è¡Œæ“ä½œï¼ˆä¼ é€’å…ƒç´ åˆ—è¡¨ï¼‰
            try:
                # å¤„ç†æ‰¹é‡æ‰§è¡Œ
                if decision.get('action') == 'BATCH_EXECUTE':
                    await self._handle_batch_execute(
                        page=page,
                        context=context,
                        decision=decision,
                        query=query
                    )
                    # æ‰¹é‡æ‰§è¡Œå®Œæˆåç»§ç»­å¾ªç¯ï¼Œè®©LLMåšæ€»ç»“
                    continue

                # æ‰§è¡Œæ“ä½œå¹¶è·å–å¯èƒ½æ›´æ–°çš„ page å¯¹è±¡ï¼ˆæ–°æ ‡ç­¾é¡µï¼‰
                page = await self._execute_action(page, decision, elements)
                new_url = page.url

                # ğŸ†• æ›´æ–°é¡µé¢å›¾è°±
                if new_url != current_url:
                    # æ¨æµ‹é¡µé¢ç±»å‹
                    page_type = "other"
                    if "list" in new_url or "search" in new_url or "disclosure" in new_url:
                        page_type = "list"
                    elif "detail" in new_url or "article" in new_url or "notice" in new_url:
                        page_type = "detail"

                    page_title = await page.title()
                    self.site_graph.add_or_update_page(
                        url=new_url,
                        page_type=page_type,
                        title=page_title,
                        description=f"ä» {self.site_graph.current_url[:40] if self.site_graph.current_url else 'unknown'} å¯¼èˆªè€Œæ¥"
                    )
                    self.site_graph.mark_navigation(current_url, new_url, decision.get('action'))

                # è®°å½•æ“ä½œç»“æœåˆ°å¯¹è¯å†å²
                if new_url != current_url:
                    result_message = f"æ“ä½œå·²æ‰§è¡Œï¼Œé¡µé¢å·²è·³è½¬åˆ°: {new_url}"
                    logger.info(f"ğŸ”„ é¡µé¢è·³è½¬: {current_url[:50]}... -> {new_url[:50]}...")
                else:
                    result_message = f"æ“ä½œå·²æ‰§è¡Œå®Œæˆ"
                    logger.info(f"âœ… æ“ä½œå®Œæˆï¼ˆé¡µé¢æœªè·³è½¬ï¼‰")

                self.conversation_history.append({
                    "role": "user",
                    "content": result_message
                })

                # âš ï¸ æ³¨æ„ï¼šä¸åœ¨è¿™é‡Œæ›´æ–° current_urlï¼Œè€Œæ˜¯åœ¨ä¸‹ä¸€æ¬¡å¾ªç¯å¼€å§‹æ—¶ç»Ÿä¸€æ›´æ–°
                # è¿™æ ·ç¡®ä¿æˆªå›¾æ—¶çš„ URL æ€»æ˜¯æœ€æ–°çš„
                logger.info(f"æ“ä½œç»“æœå·²è®°å½•åˆ°å¯¹è¯å†å²")

                # ğŸ†• æ£€æŸ¥å½“å‰å­ä»»åŠ¡æ˜¯å¦å®Œæˆï¼ˆè®© LLM åˆ¤æ–­ï¼‰
                await self._check_subtask_completion(page)

            except Exception as e:
                logger.error(f"æ‰§è¡Œæ“ä½œå¤±è´¥: {e}")
                step_record['error'] = str(e)

                # è®°å½•é”™è¯¯åˆ°å¯¹è¯å†å²
                self.conversation_history.append({
                    "role": "user",
                    "content": f"æ“ä½œæ‰§è¡Œå¤±è´¥: {str(e)}"
                })
                # ç»§ç»­ä¸‹ä¸€æ­¥

        # è¾¾åˆ°æœ€å¤§æ­¥æ•°
        logger.warning(f"è¾¾åˆ°æœ€å¤§æ­¥æ•° {self.max_steps}ï¼Œä»»åŠ¡æœªå®Œæˆ")

        # ä»»åŠ¡å¤±è´¥æ—¶ä¹Ÿè‡ªåŠ¨å¯¼å‡ºå¯¹è¯å†å²
        history_path = self.export_conversation_history()
        logger.info(f"å¯¹è¯å†å²å·²è‡ªåŠ¨å¯¼å‡º: {history_path}")

        return {
            'success': False,
            'error': f'è¾¾åˆ°æœ€å¤§æ­¥æ•° {self.max_steps}ï¼Œæœªæ‰¾åˆ°ç­”æ¡ˆ',
            'steps': steps,
            'conversation_history_path': history_path
        }

    async def _analyze_page(
        self,
        screenshot_base64: str,
        elements: List[dict],
        query: str,
        current_url: str,
        step_count: int,
        task_id: int
    ) -> Dict:
        """
        è°ƒç”¨å¤šæ¨¡æ€æ¨¡å‹åˆ†æå½“å‰é¡µé¢ï¼ˆè§†è§‰æ ‡æ³¨æ–¹æ¡ˆï¼Œæ”¯æŒå¯¹è¯å†å²ï¼‰

        Args:
            screenshot_base64: æ ‡æ³¨åçš„æˆªå›¾ï¼ˆbase64ï¼‰
            elements: å¯äº¤äº’å…ƒç´ åˆ—è¡¨
            query: ç”¨æˆ·é—®é¢˜
            current_url: å½“å‰URL
            step_count: å½“å‰æ­¥æ•°
            task_id: ä»»åŠ¡IDï¼ˆç”¨äºä¿å­˜è°ƒè¯•ä¿¡æ¯ï¼‰

        Returns:
            {
                'action': 'CLICK' | 'TYPE' | 'SCROLL' | 'TASK_COMPLETE',
                'reasoning': str,
                'element_id': int (for CLICK/TYPE),
                'text': str (for TYPE),
                'scroll_amount': int (for SCROLL),
                'summary': str (for TASK_COMPLETE),
                'citations': List[str] (for TASK_COMPLETE)
            }
        """

        # æ£€æµ‹æœ€è¿‘çš„é‡å¤æ“ä½œ
        repeated_actions_warning = ""
        if len(self.recent_actions) >= 2:
            # ç»Ÿè®¡æœ€è¿‘æ“ä½œä¸­çš„é‡å¤
            action_counts = {}
            for action, elem_id, _ in self.recent_actions:  # _ è¡¨ç¤ºå¿½ç•¥text
                key = (action, elem_id)
                action_counts[key] = action_counts.get(key, 0) + 1

            # æ‰¾å‡ºé‡å¤çš„æ“ä½œ
            repeated = [(action, elem_id, count) for (action, elem_id), count in action_counts.items() if count >= 2]
            if repeated:
                warnings = []
                for action, elem_id, count in repeated:
                    warnings.append(f"  - {action} å…ƒç´ {elem_id}ï¼ˆå·²é‡å¤{count}æ¬¡ï¼‰")
                repeated_actions_warning = f"\n\nâš ï¸ **è­¦å‘Šï¼šæ£€æµ‹åˆ°é‡å¤æ“ä½œ**ï¼š\n" + "\n".join(warnings) + "\nè¯·å°è¯•ä¸åŒçš„ç­–ç•¥ï¼Œé¿å…æ— æ•ˆçš„é‡å¤æ“ä½œï¼"

        # ğŸ†• è·å–ä»»åŠ¡è¿›åº¦å’Œå¯¼èˆªä¸Šä¸‹æ–‡
        task_progress = self.task_manager.get_progress_summary()
        navigation_context = self.site_graph.get_navigation_context()

        # æ„å»ºå½“å‰æ­¥éª¤çš„ç”¨æˆ·æ¶ˆæ¯ï¼ˆåŒ…å«åŠ¨æ€ä¿¡æ¯ï¼‰
        # ä¼˜åŒ–ï¼šä¸å†å‘é€å…ƒç´ åˆ—è¡¨æ–‡æœ¬ï¼Œåªå‘é€æˆªå›¾ï¼ˆå·²æ ‡æ³¨å…ƒç´ ç¼–å·ï¼‰
        # è¿™æ ·å¯ä»¥å¤§å¹…å‡å°‘ token æ¶ˆè€—ï¼ˆçº¦ 80-90%ï¼‰ï¼Œå……åˆ†åˆ©ç”¨ GPT-5 çš„è§†è§‰èƒ½åŠ›

        # è·å–ä¸‹è½½ç›®å½•æ–‡ä»¶ä¿¡æ¯
        downloads_info = self._get_downloads_info()

        # è·å–ä¸‹è½½æ–‡ä»¶æ•°é‡ï¼Œç”¨äºå†³å®šæ˜¯å¦æ˜¾ç¤ºæç¤º
        download_count = len(self.downloaded_files) if hasattr(self, 'downloaded_files') else 0
        download_hint = ""
        if download_count > 0:
            download_hint = f"\n\nğŸ’¡ **é‡è¦æç¤º**: å·²æœ‰ {download_count} ä¸ªæ–‡ä»¶ä¸‹è½½æˆåŠŸï¼å¦‚æœä»»åŠ¡ç›®æ ‡æ˜¯ä¸‹è½½æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥ä¸Šæ–¹çš„ã€ä¸‹è½½ç›®å½•ã€‘ä¿¡æ¯ï¼Œç¡®è®¤æ–‡ä»¶æ˜¯å¦ç¬¦åˆè¦æ±‚ã€‚"

        # åªåœ¨é¦–è½®ï¼ˆstep_count == 1ï¼‰å‘é€æ€»ä½“ä»»åŠ¡ä¿¡æ¯
        if step_count == 1:
            current_step_info = f"""**æ€»ä½“ä»»åŠ¡**: {query}

{task_progress}

{navigation_context}

{downloads_info}

**å½“å‰é¡µé¢URL**: {current_url}
**å½“å‰æ­¥æ•°**: {step_count}/{self.max_steps}
**å¯äº¤äº’å…ƒç´ æ•°é‡**: {len(elements)} ä¸ª{repeated_actions_warning}{download_hint}

è¯·åˆ†æè¿™ä¸ªæ ‡æ³¨åçš„é¡µé¢æˆªå›¾ï¼ˆçº¢è‰²æ¡†å’Œæ•°å­—è¡¨ç¤ºå¯äº¤äº’å…ƒç´ çš„ç¼–å·ï¼‰ï¼Œå†³å®šä¸‹ä¸€æ­¥æ“ä½œã€‚æ³¨æ„ï¼š**å¿…é¡»å®Œæˆå½“å‰å­ä»»åŠ¡åå†è¿›è¡Œä¸‹ä¸€ä¸ª**ï¼"""
        else:
            # åç»­æ­¥éª¤åªå‘é€åŠ¨æ€å˜åŒ–çš„ä¿¡æ¯
            current_step_info = f"""{task_progress}

{navigation_context}

{downloads_info}

**å½“å‰é¡µé¢URL**: {current_url}
**å½“å‰æ­¥æ•°**: {step_count}/{self.max_steps}
**å¯äº¤äº’å…ƒç´ æ•°é‡**: {len(elements)} ä¸ª{repeated_actions_warning}{download_hint}

è¯·åˆ†æè¿™ä¸ªæ ‡æ³¨åçš„é¡µé¢æˆªå›¾ï¼ˆçº¢è‰²æ¡†å’Œæ•°å­—è¡¨ç¤ºå¯äº¤äº’å…ƒç´ çš„ç¼–å·ï¼‰ï¼Œå†³å®šä¸‹ä¸€æ­¥æ“ä½œã€‚æ³¨æ„ï¼š**å¿…é¡»å®Œæˆå½“å‰å­ä»»åŠ¡åå†è¿›è¡Œä¸‹ä¸€ä¸ª**ï¼"""

        # æ£€æŸ¥å¹¶å‹ç¼©å¯¹è¯å†å²ï¼ˆå¦‚æœè¶…è¿‡é˜ˆå€¼ï¼‰
        await self._compress_conversation_history()

        # æ„å»ºå®Œæ•´çš„æ¶ˆæ¯åˆ—è¡¨
        messages = []

        # 1. æ·»åŠ é™æ€ system promptï¼ˆç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶ä¼šè¢«ç¼“å­˜ï¼‰
        messages.append({
            "role": "system",
            "content": self.static_system_prompt
        })

        # 2. å¦‚æœæœ‰å†å²æ€»ç»“ï¼Œæ·»åŠ åˆ°æ¶ˆæ¯åˆ—è¡¨ä¸­
        if self.history_summary:
            messages.append({
                "role": "user",
                "content": f"**ä¹‹å‰çš„æ“ä½œå†å²æ€»ç»“**ï¼š\n{self.history_summary}"
            })
            messages.append({
                "role": "assistant",
                "content": "æˆ‘å·²äº†è§£ä¹‹å‰çš„æ“ä½œå†å²ï¼Œä¼šåŸºäºè¿™äº›ä¿¡æ¯ç»§ç»­æ‰§è¡Œä»»åŠ¡ã€‚"
            })

        # 3. æ·»åŠ å†å²å¯¹è¯ï¼ˆå‹ç¼©åçš„æœ€è¿‘å¯¹è¯ï¼‰
        messages.extend(self.conversation_history)

        # 4. æ·»åŠ å½“å‰æ­¥éª¤çš„æ¶ˆæ¯
        messages.append({
            "role": "user",
            "content": [
                {
                    "type": "input_text",
                    "text": current_step_info
                },
                {
                    "type": "input_image",
                    "image_url": f"data:image/png;base64,{screenshot_base64}"
                }
            ]
        })

        # ğŸ’¾ ä¿å­˜å‘é€ç»™ LLM çš„æ¶ˆæ¯åˆ°æœ¬åœ°ï¼ˆç”¨äºè°ƒè¯•å’Œ token åˆ†æï¼‰
        try:
            debug_dir = Path("debug_messages")
            debug_dir.mkdir(exist_ok=True)

            # ä¿å­˜æ¶ˆæ¯å†…å®¹ï¼ˆä¸åŒ…å«base64å›¾ç‰‡ï¼Œå¤ªå¤§ï¼‰
            messages_for_debug = []
            for msg in messages:
                if isinstance(msg.get('content'), list):
                    # å¤„ç†åŒ…å«å›¾ç‰‡çš„æ¶ˆæ¯
                    debug_content = []
                    for item in msg['content']:
                        if item.get('type') == 'input_image':
                            debug_content.append({
                                'type': 'input_image',
                                'image_url': '[BASE64_IMAGE_OMITTED]',
                                'image_size_estimate': f'~{len(item.get("image_url", ""))} chars'
                            })
                        else:
                            debug_content.append(item)
                    messages_for_debug.append({
                        'role': msg['role'],
                        'content': debug_content
                    })
                else:
                    messages_for_debug.append(msg)

            debug_file = debug_dir / f"task_{task_id}_step_{step_count}_messages.json"
            with open(debug_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'task_id': task_id,
                    'step_count': step_count,
                    'current_url': current_url,
                    'messages': messages_for_debug,
                    'elements_count': len(elements)
                }, f, ensure_ascii=False, indent=2)

            logger.info(f"ğŸ’¾ å·²ä¿å­˜æ¶ˆæ¯åˆ°: {debug_file}")
        except Exception as e:
            logger.warning(f"ä¿å­˜è°ƒè¯•æ¶ˆæ¯å¤±è´¥: {e}")

        try:
            response = self.client.responses.parse(
                model="gpt-5-mini-2025-08-07",
                input=messages,
                text_format=BrowserDecision,
                max_output_tokens=50000,
            )

            # ç›´æ¥è·å–è§£æåçš„ç»“æ„åŒ–å¯¹è±¡
            decision_obj: BrowserDecision = response.output_parsed
            decision_dict = decision_obj.model_dump()

            # æ¸…ç† text å­—æ®µä¸­å¯èƒ½çš„ JSON æ ¼å¼æ®‹ç•™ç¬¦å·
            # æ³¨æ„ï¼šå³ä½¿ä½¿ç”¨ OpenAI Structured Outputsï¼Œæ¨¡å‹ä»å¯èƒ½åœ¨å­—ç¬¦ä¸²å­—æ®µä¸­
            # "æ³„æ¼"JSON ç»“æ„ç¬¦å·ï¼ˆå¦‚ "300866}"ï¼‰ï¼Œå› ä¸ºè¿™åœ¨ JSON è¯­æ³•ä¸Šæ˜¯åˆæ³•çš„
            if decision_dict.get('text'):
                import re
                original_text = decision_dict['text']
                # ç§»é™¤æœ«å°¾çš„ JSON æ ¼å¼ç¬¦å·ï¼ˆå¦‚ }, ]{, }{ ç­‰ç»„åˆï¼‰
                text = original_text.strip()
                # ä½¿ç”¨æ­£åˆ™ç§»é™¤æœ«å°¾çš„æ‰€æœ‰ JSON æ ¼å¼å­—ç¬¦ï¼ˆåŒ…æ‹¬ { [ } ] , " ' `ï¼‰
                text = re.sub(r'[}\]\[\{,\"\'`]+$', '', text).strip()

                if text != original_text:
                    logger.warning(f"ğŸ§¹ æ¸…ç† text å­—æ®µ: '{original_text}' -> '{text}'")

                decision_dict['text'] = text

            # ä¿å­˜å½“å‰ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²ï¼ˆä¸åŒ…å«å›¾ç‰‡ï¼ŒèŠ‚çœç©ºé—´ï¼‰
            self.conversation_history.append({
                "role": "user",
                "content": [
                    {
                        "type": "input_text",
                        "text": current_step_info + "\n[æˆªå›¾å·²çœç•¥]"
                    }
                ]
            })

            # ä¿å­˜æ¨¡å‹å“åº”åˆ°å†å²
            assistant_message = f"""å†³ç­–: {decision_dict['action']}
æ¨ç†: {decision_dict['reasoning']}"""

            if decision_dict.get('element_id'):
                assistant_message += f"\nå…ƒç´ ID: {decision_dict['element_id']}"
            if decision_dict.get('text'):
                assistant_message += f"\nè¾“å…¥æ–‡æœ¬: {decision_dict['text']}"
            if decision_dict.get('scroll_amount'):
                assistant_message += f"\næ»šåŠ¨è·ç¦»: {decision_dict['scroll_amount']}px"

            self.conversation_history.append({
                "role": "assistant",
                "content": assistant_message
            })

            # è®°å½•æ“ä½œåˆ°æœ€è¿‘æ“ä½œåˆ—è¡¨ï¼ˆç”¨äºæ£€æµ‹é‡å¤ï¼‰
            if decision_dict['action'] != 'TASK_COMPLETE':
                self._record_action(
                    decision_dict['action'],
                    decision_dict.get('element_id'),
                    decision_dict.get('text')
                )

            logger.info(f"å¯¹è¯å†å²é•¿åº¦: {len(self.conversation_history)} æ¡æ¶ˆæ¯")

            return decision_dict

        except Exception as e:
            logger.error(f"è§†è§‰æ¨¡å‹è°ƒç”¨å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()

            # è¿”å›é»˜è®¤å†³ç­–
            return {
                'action': 'TASK_COMPLETE',
                'reasoning': f'æ¨¡å‹è°ƒç”¨å¤±è´¥: {e}',
                'summary': 'æŠ±æ­‰ï¼Œæ— æ³•å®Œæˆä»»åŠ¡',
                'citations': []
            }

    async def _check_subtask_completion(self, page: Page):
        """
        æ£€æŸ¥å½“å‰å­ä»»åŠ¡æ˜¯å¦å®Œæˆï¼ˆä½¿ç”¨ LLM åˆ¤æ–­ï¼‰

        è®© LLM åŸºäºå½“å‰é¡µé¢å†…å®¹åˆ¤æ–­å­ä»»åŠ¡æ˜¯å¦è¾¾æˆç›®æ ‡
        """
        current_subtask = self.task_manager.get_current_subtask()
        if not current_subtask or current_subtask.status == "completed":
            return

        # æ ‡è®°ä¸ºè¿›è¡Œä¸­ï¼ˆå¦‚æœè¿˜æ²¡æ ‡è®°ï¼‰
        if current_subtask.status == "pending":
            self.task_manager.mark_current_subtask_in_progress()

        # è·å–é¡µé¢æ–‡æœ¬æ‘˜è¦
        try:
            page_text = await page.evaluate("""
                () => {
                    // æå–é¡µé¢ä¸»è¦æ–‡æœ¬ï¼ˆå‰2000å­—ç¬¦ï¼‰
                    return document.body.innerText.substring(0, 2000);
                }
            """)
        except:
            page_text = ""

        # ä½¿ç”¨ LLM åˆ¤æ–­å­ä»»åŠ¡æ˜¯å¦å®Œæˆï¼ˆåŸºäº success_criteriaï¼‰
        try:
            # æ„å»ºéªŒè¯æ¡ä»¶æ–‡æœ¬
            criteria_text = "\n".join([f"- {criterion}" for criterion in current_subtask.success_criteria])

            response = self.client.responses.parse(
                model="gpt-5-mini-2025-08-07",
                input=[
                    {
                        "role": "system",
                        "content": """ä½ æ˜¯ä¸€ä¸ªä»»åŠ¡çŠ¶æ€éªŒè¯ä¸“å®¶ã€‚æ ¹æ®ç›®æ ‡çŠ¶æ€çš„éªŒè¯æ¡ä»¶ï¼Œåˆ¤æ–­å½“å‰æ˜¯å¦å·²è¾¾æˆç›®æ ‡ã€‚

**æ ¸å¿ƒåŸåˆ™**ï¼š
- éªŒè¯**çŠ¶æ€æ˜¯å¦åŸºæœ¬è¾¾æˆ**ï¼Œè€Œä¸æ˜¯è¦æ±‚100%å®Œç¾
- æ ¹æ®æä¾›çš„ success_criteriaï¼ˆå®Œæˆæ¡ä»¶ï¼‰è¯„ä¼°ï¼Œä½†**å…è®¸å¼¹æ€§**
- å¦‚æœ**ä¸»è¦ç›®æ ‡æ˜æ˜¾å·²è¾¾æˆ**ï¼ˆå¦‚æ‰¾åˆ°äº†åˆ—è¡¨é¡µé¢ã€ä¸‹è½½äº†æ–‡ä»¶ï¼‰ï¼Œå°±å¯ä»¥åˆ¤æ–­ä¸ºå®Œæˆ
- ä¸è¦å› ä¸ºæ¬¡è¦ç»†èŠ‚æœªæ»¡è¶³å°±å¡ä½æµç¨‹

**åˆ¤æ–­ä¾æ®**ï¼š
- å½“å‰é¡µé¢çš„ URLã€æ ‡é¢˜ã€å¯è§å†…å®¹
- æ˜¯å¦ç¬¦åˆç›®æ ‡çŠ¶æ€çš„ä¸»è¦æè¿°
- å…³é”®éªŒè¯æ¡ä»¶æ˜¯å¦æ»¡è¶³ï¼ˆå…è®¸éƒ¨åˆ†æ»¡è¶³ï¼‰

**ç¤ºä¾‹1**ï¼š
ç›®æ ‡çŠ¶æ€ï¼š"å·²æ‰¾åˆ°å…¬å‘Šåˆ—è¡¨é¡µé¢"
éªŒè¯æ¡ä»¶ï¼š["é¡µé¢URLåŒ…å«'å…¬å‘Š'å…³é”®è¯", "é¡µé¢ä¸­æœ‰å¤šæ¡å…¬å‘Š"]
åˆ¤æ–­ï¼šå¦‚æœå½“å‰é¡µé¢æ˜¾ç¤ºäº†æ˜æ˜¾çš„å…¬å‘Šåˆ—è¡¨ï¼ˆå³ä½¿URLä¸å®Œå…¨åŒ¹é…ï¼‰ï¼Œå°±åˆ¤æ–­ä¸º**å·²å®Œæˆ**

**ç¤ºä¾‹2**ï¼š
ç›®æ ‡çŠ¶æ€ï¼š"å·²ä¸‹è½½è‡³å°‘5æ¡å…¬å‘ŠPDFåˆ°æœ¬åœ°"
éªŒè¯æ¡ä»¶ï¼š["æœ¬åœ°å­˜åœ¨è‡³å°‘5ä¸ªPDFæ–‡ä»¶"]
åˆ¤æ–­ï¼šå³ä½¿åªä¸‹è½½äº†3ä¸ªPDFï¼Œå¦‚æœä¸‹è½½è¿‡ç¨‹æ­£åœ¨è¿›è¡Œä¸­ï¼Œä¹Ÿå¯ä»¥ç»§ç»­ï¼Œä¸è¦æ­»æ¿åœ°å¡åœ¨"å¿…é¡»5ä¸ª"

**é‡è¦**ï¼šä¿æŒ**å®ç”¨ä¸»ä¹‰**ï¼Œä¼˜å…ˆè®©ä»»åŠ¡ç»§ç»­æ¨è¿›ï¼Œè€Œä¸æ˜¯è¿½æ±‚å®Œç¾éªŒè¯ã€‚"""
                    },
                    {
                        "role": "user",
                        "content": f"""**ç›®æ ‡çŠ¶æ€**: {current_subtask.description}

**éªŒè¯æ¡ä»¶**ï¼š
{criteria_text}

**å½“å‰é¡µé¢ä¿¡æ¯**ï¼š
- URL: {page.url}
- æ ‡é¢˜: {await page.title()}
- å†…å®¹æ‘˜è¦: {page_text[:800]}...

**æœ¬åœ°ä¸‹è½½æ–‡ä»¶ä¿¡æ¯**ï¼š
{self._get_downloads_info()}

**é‡è¦æç¤º**ï¼š
- å¦‚æœä»»åŠ¡æ¶‰åŠæ–‡ä»¶ä¸‹è½½ï¼Œè¯·æ£€æŸ¥"æœ¬åœ°ä¸‹è½½æ–‡ä»¶ä¿¡æ¯"éƒ¨åˆ†
- å¦‚æœä¸‹è½½ç›®å½•ä¸­å·²æœ‰ç¬¦åˆè¦æ±‚çš„æ–‡ä»¶ï¼ˆæ–‡ä»¶åã€å¤§å°ã€æ—¶é—´åˆç†ï¼‰ï¼Œå³å¯è®¤ä¸ºä¸‹è½½æˆåŠŸ
- ä¸éœ€è¦éªŒè¯æ–‡ä»¶"å¯æ‰“å¼€"ï¼Œæ–‡ä»¶å­˜åœ¨ä¸”å¤§å°>0å³å¯

è¯·åˆ¤æ–­ç›®æ ‡çŠ¶æ€æ˜¯å¦å·²è¾¾æˆï¼ˆæ‰€æœ‰å…³é”®éªŒè¯æ¡ä»¶æ˜¯å¦æ»¡è¶³ï¼‰ã€‚"""
                    }
                ],
                text_format=SubtaskCompletionCheck,
                max_output_tokens=2000,
            )

            # è·å–ç»“æ„åŒ–è¾“å‡º
            check: SubtaskCompletionCheck = response.output_parsed

            if check.completed:
                logger.info(f"âœ… å­ä»»åŠ¡ #{current_subtask.id} å·²å®Œæˆ: {current_subtask.description}")
                logger.info(f"   åŸå› : {check.reason}")

                self.task_manager.mark_current_subtask_complete(
                    result=check.reason
                )
            else:
                logger.info(f"â³ å­ä»»åŠ¡ #{current_subtask.id} å°šæœªå®Œæˆ: {check.reason}")

        except Exception as e:
            logger.warning(f"å­ä»»åŠ¡å®Œæˆæ£€æŸ¥å¤±è´¥: {e}ï¼Œç»§ç»­æ‰§è¡Œ")

    async def _execute_action(
        self,
        page: Page,
        decision: Dict,
        elements: List[dict],
        max_retries: int = 3
    ) -> Page:
        """
        æ‰§è¡Œæµè§ˆå™¨æ“ä½œï¼ˆè§†è§‰æ ‡æ³¨æ–¹æ¡ˆ - åŸºäºelement_idï¼‰

        Args:
            page: Playwrighté¡µé¢å¯¹è±¡
            decision: å†³ç­–å­—å…¸ï¼ˆåŒ…å«actionå’Œelement_idï¼‰
            elements: å¯äº¤äº’å…ƒç´ åˆ—è¡¨
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°

        Returns:
            Page: å½“å‰æ´»åŠ¨çš„é¡µé¢å¯¹è±¡ï¼ˆå¯èƒ½æ˜¯æ–°æ‰“å¼€çš„æ ‡ç­¾é¡µï¼‰
        """
        action = decision.get('action')

        if action == 'CLICK':
            element_id = decision.get('element_id')
            if not element_id:
                logger.warning("CLICKæ“ä½œç¼ºå°‘element_id")
                return

            # æ‰¾åˆ°å¯¹åº”çš„å…ƒç´ 
            elem = next((e for e in elements if e['id'] == element_id), None)
            if not elem:
                logger.error(f"æœªæ‰¾åˆ°å…ƒç´ ID: {element_id}")
                return

            # ä½¿ç”¨å”¯ä¸€selectorï¼ˆ100%ä¸€ä¸€å¯¹åº”ï¼‰
            unique_selector = elem.get('uniqueSelector') or elem['selector']
            fallback_selector = elem['selector']

            logger.info(f"ç‚¹å‡»å…ƒç´  [{elem['id']}]: {elem['text']}")
            logger.info(f"  å”¯ä¸€å®šä½ç¬¦: {unique_selector}")

            # è·å–å½“å‰æµè§ˆå™¨ä¸Šä¸‹æ–‡ï¼ˆç”¨äºæ£€æµ‹æ–°æ ‡ç­¾é¡µï¼‰
            context = page.context
            initial_pages = context.pages

            # é‡è¯•æœºåˆ¶
            for attempt in range(max_retries):
                try:
                    # ä¼˜å…ˆä½¿ç”¨å”¯ä¸€selector
                    locator = page.locator(unique_selector)

                    # éªŒè¯å…ƒç´ å­˜åœ¨ä¸”å”¯ä¸€
                    count = await locator.count()
                    if count == 0:
                        logger.warning(f"å”¯ä¸€selectoræœªæ‰¾åˆ°å…ƒç´ ï¼Œå°è¯•fallback")
                        locator = page.locator(fallback_selector).first
                    elif count > 1:
                        logger.warning(f"âš ï¸  æ‰¾åˆ°{count}ä¸ªåŒ¹é…å…ƒç´ ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ª")
                        locator = locator.first
                    else:
                        logger.info(f"âœ“ å”¯ä¸€å®šä½æˆåŠŸ")

                    # ç­‰å¾…å…ƒç´ å¯è§
                    await locator.wait_for(state='visible', timeout=5000)

                    # æ»šåŠ¨åˆ°è§†å›¾
                    await locator.scroll_into_view_if_needed()
                    await asyncio.sleep(0.5)

                    # æ‰§è¡Œç‚¹å‡»ï¼ˆå¯èƒ½æ‰“å¼€æ–°æ ‡ç­¾é¡µï¼‰
                    await locator.click()
                    logger.info(f"âœ… ç‚¹å‡»æˆåŠŸ")

                    # ç­‰å¾…é¡µé¢å“åº”
                    try:
                        await page.wait_for_load_state('networkidle', timeout=10000)
                    except:
                        # å¯èƒ½æ²¡æœ‰ç½‘ç»œè¯·æ±‚ï¼Œç­‰å¾…DOMå˜åŒ–
                        await asyncio.sleep(1)

                    # æ£€æŸ¥æ˜¯å¦æ‰“å¼€äº†æ–°æ ‡ç­¾é¡µ
                    await asyncio.sleep(0.5)  # ç­‰å¾…æ–°æ ‡ç­¾é¡µå®Œå…¨æ‰“å¼€
                    current_pages = context.pages

                    if len(current_pages) > len(initial_pages):
                        # æœ‰æ–°æ ‡ç­¾é¡µæ‰“å¼€ï¼Œåˆ‡æ¢åˆ°æœ€æ–°çš„æ ‡ç­¾é¡µ
                        new_page = current_pages[-1]
                        logger.info(f"ğŸ†• æ£€æµ‹åˆ°æ–°æ ‡ç­¾é¡µï¼Œåˆ‡æ¢åˆ°: {new_page.url}")

                        # ç­‰å¾…æ–°é¡µé¢åŠ è½½
                        try:
                            await new_page.wait_for_load_state('networkidle', timeout=10000)
                        except:
                            await asyncio.sleep(1)

                        # ğŸ“¥ ä¸ºæ–°æ ‡ç­¾é¡µè®¾ç½®ä¸‹è½½ç›‘å¬å™¨
                        self._setup_download_listener(new_page)

                        return new_page

                    return page

                except Exception as e:
                    logger.warning(f"ç‚¹å‡»å°è¯• {attempt + 1}/{max_retries} å¤±è´¥: {str(e)[:100]}")
                    if attempt < max_retries - 1:
                        await asyncio.sleep(1)
                    else:
                        logger.error(f"âŒ ç‚¹å‡»æœ€ç»ˆå¤±è´¥: {e}")

            return page

        elif action == 'TYPE':
            element_id = decision.get('element_id')
            text = decision.get('text', '')

            if not element_id:
                logger.warning("TYPEæ“ä½œç¼ºå°‘element_id")
                return

            # æ‰¾åˆ°å¯¹åº”çš„å…ƒç´ 
            elem = next((e for e in elements if e['id'] == element_id), None)
            if not elem:
                logger.error(f"æœªæ‰¾åˆ°å…ƒç´ ID: {element_id}")
                return

            # ä½¿ç”¨å”¯ä¸€selectorï¼ˆ100%ä¸€ä¸€å¯¹åº”ï¼‰
            unique_selector = elem.get('uniqueSelector') or elem['selector']
            fallback_selector = elem['selector']

            logger.info(f"åœ¨å…ƒç´  [{elem['id']}] è¾“å…¥: {text}")
            logger.info(f"  å”¯ä¸€å®šä½ç¬¦: {unique_selector}")

            # é‡è¯•æœºåˆ¶
            for attempt in range(max_retries):
                try:
                    # ä¼˜å…ˆä½¿ç”¨å”¯ä¸€selector
                    locator = page.locator(unique_selector)

                    # éªŒè¯å…ƒç´ å­˜åœ¨ä¸”å”¯ä¸€
                    count = await locator.count()
                    if count == 0:
                        logger.warning(f"å”¯ä¸€selectoræœªæ‰¾åˆ°å…ƒç´ ï¼Œå°è¯•fallback")
                        locator = page.locator(fallback_selector).first
                    elif count > 1:
                        logger.warning(f"âš ï¸  æ‰¾åˆ°{count}ä¸ªåŒ¹é…å…ƒç´ ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ª")
                        locator = locator.first
                    else:
                        logger.info(f"âœ“ å”¯ä¸€å®šä½æˆåŠŸ")

                    # ç­‰å¾…å…ƒç´ å¯è§
                    await locator.wait_for(state='visible', timeout=5000)

                    # æ»šåŠ¨åˆ°è§†å›¾
                    await locator.scroll_into_view_if_needed()
                    await asyncio.sleep(0.5)

                    # ç‚¹å‡»æ¿€æ´»è¾“å…¥æ¡†
                    await locator.click()
                    await asyncio.sleep(0.2)

                    # æ¸…ç©ºå¹¶è¾“å…¥
                    await locator.fill(text)
                    logger.info(f"âœ… è¾“å…¥æˆåŠŸ")

                    # æŒ‰å›è½¦ï¼ˆé€šå¸¸ç”¨äºæœç´¢ï¼‰
                    await locator.press('Enter')

                    # ç­‰å¾…é¡µé¢å“åº”
                    try:
                        await page.wait_for_load_state('networkidle', timeout=10000)
                    except:
                        await asyncio.sleep(1)

                    return page

                except Exception as e:
                    logger.warning(f"è¾“å…¥å°è¯• {attempt + 1}/{max_retries} å¤±è´¥: {str(e)[:100]}")
                    if attempt < max_retries - 1:
                        await asyncio.sleep(1)
                    else:
                        logger.error(f"âŒ è¾“å…¥å¤±è´¥: {e}")

            return page

        elif action == 'SCROLL':
            scroll_amount = decision.get('scroll_amount', 500)
            element_id = decision.get('element_id')  # å¯é€‰ï¼šæŒ‡å®šè¦æ»šåŠ¨çš„å…ƒç´ 

            if element_id:
                # æ»šåŠ¨ç‰¹å®šå…ƒç´ ï¼ˆå¦‚ iframeã€å¯æ»šåŠ¨ divï¼‰
                elem = next((e for e in elements if e['id'] == element_id), None)
                if not elem:
                    logger.error(f"æœªæ‰¾åˆ°è¦æ»šåŠ¨çš„å…ƒç´ ID: {element_id}")
                    return page

                logger.info(f"æ»šåŠ¨å…ƒç´  [{elem['id']}]: {elem['text']} ({scroll_amount}px)")

                try:
                    unique_selector = elem.get('uniqueSelector') or elem['selector']
                    # æ»šåŠ¨ç‰¹å®šå…ƒç´ 
                    await page.locator(unique_selector).evaluate(
                        f"element => element.scrollBy(0, {scroll_amount})"
                    )
                    logger.info(f"âœ… å…ƒç´ æ»šåŠ¨æˆåŠŸ")
                    await asyncio.sleep(1)
                except Exception as e:
                    logger.error(f"å…ƒç´ æ»šåŠ¨å¤±è´¥: {e}")

            else:
                # æ»šåŠ¨æ•´ä¸ªé¡µé¢ï¼ˆé»˜è®¤è¡Œä¸ºï¼‰
                logger.info(f"æ»šåŠ¨é¡µé¢: {scroll_amount}px")

                try:
                    # ä½¿ç”¨ window.scrollBy æ˜ç¡®æ»šåŠ¨é¡µé¢ï¼Œé¿å…æ­§ä¹‰
                    await page.evaluate(f"window.scrollBy(0, {scroll_amount})")
                    logger.info(f"âœ… é¡µé¢æ»šåŠ¨æˆåŠŸ")
                    await asyncio.sleep(1)  # ç­‰å¾…å†…å®¹åŠ è½½
                except Exception as e:
                    logger.error(f"é¡µé¢æ»šåŠ¨å¤±è´¥: {e}")

            return page

        elif action == 'BACK':
            logger.info("ğŸ”™ æµè§ˆå™¨åé€€")

            try:
                # æ£€æŸ¥å½“å‰é¡µé¢æ˜¯å¦æœ‰å†å²è®°å½•
                # å¦‚æœæ˜¯æ–°æ‰“å¼€çš„æ ‡ç­¾é¡µï¼Œå¯èƒ½æ²¡æœ‰å†å²è®°å½•
                context = page.context
                all_pages = context.pages

                # å°è¯•åé€€
                try:
                    await page.go_back(wait_until='networkidle', timeout=10000)
                    logger.info(f"âœ… å·²è¿”å›ä¸Šä¸€é¡µ: {page.url}")
                    return page
                except Exception as e:
                    # åé€€å¤±è´¥ï¼Œå¯èƒ½æ˜¯æ–°æ ‡ç­¾é¡µæˆ–å·²ç»åœ¨ç¬¬ä¸€é¡µ
                    logger.warning(f"âš ï¸  åé€€å¤±è´¥: {e}")

                    # å¦‚æœæœ‰å¤šä¸ªæ ‡ç­¾é¡µï¼Œå¯èƒ½å½“å‰æ˜¯æ–°æ‰“å¼€çš„æ ‡ç­¾é¡µ
                    if len(all_pages) > 1:
                        logger.info("ğŸ”„ å½“å‰é¡µé¢å¯èƒ½æ˜¯æ–°æ ‡ç­¾é¡µï¼Œå°è¯•å…³é—­å¹¶è¿”å›ä¸Šä¸€ä¸ªæ ‡ç­¾é¡µ")

                        # ä¿å­˜å½“å‰é¡µé¢ç´¢å¼•
                        current_index = all_pages.index(page)

                        # å…³é—­å½“å‰æ ‡ç­¾é¡µ
                        await page.close()
                        logger.info("âœ… å·²å…³é—­å½“å‰æ ‡ç­¾é¡µ")

                        # è¿”å›åˆ°ä¸Šä¸€ä¸ªæ ‡ç­¾é¡µï¼ˆå¦‚æœå½“å‰æ˜¯æœ€åä¸€ä¸ªï¼Œè¿”å›å€’æ•°ç¬¬äºŒä¸ªï¼‰
                        if current_index > 0:
                            previous_page = all_pages[current_index - 1]
                        else:
                            # å½“å‰æ˜¯ç¬¬ä¸€ä¸ªï¼Œè¿”å›ç°åœ¨çš„ç¬¬ä¸€ä¸ªï¼ˆåŸæ¥çš„ç¬¬äºŒä¸ªï¼‰
                            previous_page = context.pages[0]

                        logger.info(f"ğŸ”™ å·²åˆ‡æ¢åˆ°ä¸Šä¸€ä¸ªæ ‡ç­¾é¡µ: {previous_page.url}")
                        return previous_page
                    else:
                        # åªæœ‰ä¸€ä¸ªæ ‡ç­¾é¡µï¼Œå·²ç»åœ¨ç¬¬ä¸€é¡µ
                        logger.warning("âš ï¸  å·²ç»åœ¨ç¬¬ä¸€é¡µæˆ–æµè§ˆå†å²ä¸ºç©º")
                        return page

            except Exception as e:
                logger.error(f"âŒ BACKæ“ä½œå¤±è´¥: {e}")
                return page

        elif action == 'FORWARD':
            logger.info("ğŸ”œ æµè§ˆå™¨å‰è¿›")

            try:
                await page.go_forward(wait_until='networkidle', timeout=10000)
                logger.info(f"âœ… å·²å‰è¿›åˆ°ä¸‹ä¸€é¡µ: {page.url}")
            except Exception as e:
                logger.error(f"å‰è¿›å¤±è´¥: {e}")
                # å³ä½¿å¤±è´¥ä¹Ÿç»§ç»­ï¼Œå¯èƒ½æ˜¯å·²ç»åœ¨æœ€åä¸€é¡µ

            return page

        elif action == 'REFRESH':
            logger.info("ğŸ”„ åˆ·æ–°é¡µé¢")

            try:
                await page.reload(wait_until='networkidle', timeout=10000)
                logger.info(f"âœ… é¡µé¢å·²åˆ·æ–°: {page.url}")
            except Exception as e:
                logger.error(f"åˆ·æ–°å¤±è´¥: {e}")

            return page

        elif action == 'CHECK_DOWNLOADS':
            logger.info("ğŸ“¥ æŸ¥çœ‹ä¸‹è½½ç›®å½•")
            # CHECK_DOWNLOADS ä¼šåœ¨ _analyze_page ä¸­è‡ªåŠ¨åœ¨ prompt é‡Œæ˜¾ç¤ºæ–‡ä»¶åˆ—è¡¨
            # è¿™é‡Œåªéœ€è¦è®°å½•ä¸€ä¸‹æ“ä½œå³å¯
            logger.info("âœ… ä¸‹è½½ç›®å½•ä¿¡æ¯å·²åŒ…å«åœ¨ä¸‹æ¬¡å†³ç­–çš„ prompt ä¸­")
            return page

        elif action == 'TASK_COMPLETE':
            logger.info("ä»»åŠ¡å®Œæˆä¿¡å·")
            # TASK_COMPLETEä¸éœ€è¦æ‰§è¡Œæ“ä½œï¼Œåªæ˜¯æ ‡è®°å®Œæˆ
            return page

        return page

    async def _handle_batch_execute(
        self,
        page: Page,
        context,
        decision: Dict,
        query: str  # noqa: ARG002 - ä¿ç•™ç”¨äºæœªæ¥æ‰©å±•
    ):
        """
        å¤„ç†æ‰¹é‡æ‰§è¡Œè¯·æ±‚

        Args:
            page: å½“å‰é¡µé¢ï¼ˆåˆ—è¡¨é¡µï¼‰
            context: Browser context
            decision: å†³ç­–å­—å…¸
            query: ç”¨æˆ·æŸ¥è¯¢
        """
        batch_element_ids = decision.get('batch_element_ids', [])
        batch_description = decision.get('batch_description', 'æ‰¹é‡æ“ä½œ')

        if not batch_element_ids:
            logger.warning("æ‰¹é‡æ‰§è¡Œç¼ºå°‘element_ids")
            return

        logger.info(f"ğŸš€ å¼€å§‹æ‰¹é‡æ‰§è¡Œ: {batch_description}")
        logger.info(f"   ç›®æ ‡å…ƒç´ : {batch_element_ids}")

        # ä½¿ç”¨æ‰¹é‡æ‰§è¡Œå¼•æ“
        results = await self.batch_engine.execute_batch(
            context=context,
            list_page=page,
            element_ids=batch_element_ids,
            description=batch_description,
            use_new_tab=True  # é»˜è®¤ä½¿ç”¨æ–°æ ‡ç­¾é¡µæ¨¡å¼
        )

        # æ ¼å¼åŒ–ç»“æœ
        summary = self._format_batch_results(results)

        # å°†ç»“æœæ·»åŠ åˆ°å¯¹è¯å†å²ï¼Œè®©LLMåšæœ€ç»ˆåˆ†æ
        self.conversation_history.append({
            "role": "user",
            "content": [
                {
                    "type": "input_text",
                    "text": f"æ‰¹é‡æ‰§è¡Œå®Œæˆã€‚\n\n{summary}\n\nè¯·åŸºäºè¿™äº›å†…å®¹å®Œæˆä»»åŠ¡ã€‚"
                }
            ]
        })

        logger.info(f"âœ… æ‰¹é‡æ‰§è¡Œå®Œæˆï¼Œå·²å°†ç»“æœè¿”å›ç»™LLM")

    def _format_batch_results(self, results: List[dict]) -> str:
        """
        å°†æ‰¹é‡æ‰§è¡Œç»“æœæ ¼å¼åŒ–ä¸ºé€‚åˆLLMé˜…è¯»çš„æ–‡æœ¬

        Args:
            results: æ‰¹é‡æ‰§è¡Œç»“æœåˆ—è¡¨

        Returns:
            æ ¼å¼åŒ–çš„æ–‡æœ¬æ‘˜è¦
        """
        success_count = sum(1 for r in results if r["status"] == "success")
        total_count = len(results)

        summary = f"æ‰¹é‡æ‰§è¡Œç»Ÿè®¡: {success_count}/{total_count} æˆåŠŸ\n\n"

        for result in results:
            if result["status"] == "success":
                data = result["data"]
                summary += f"--- é¡¹ç›® {result['index']} (å…ƒç´ #{result['element_id']}) ---\n"
                summary += f"æ ‡é¢˜: {data.get('title', 'N/A')}\n"
                summary += f"å†…å®¹æ‘˜è¦: {data.get('content', '')[:300]}...\n"
                if data.get('pdf_urls'):
                    summary += f"PDFé“¾æ¥: {', '.join(data['pdf_urls'][:3])}\n"
                summary += "\n"
            else:
                summary += f"--- é¡¹ç›® {result['index']} (å¤±è´¥) ---\n"
                summary += f"é”™è¯¯: {result.get('error', 'Unknown error')}\n\n"

        return summary

    async def _generate_highlighted_html(self, html_content: str, citations: List[str], task_id: int) -> str:
        """
        ç”Ÿæˆé«˜äº®HTMLæŠ¥å‘Š
        """
        from bs4 import BeautifulSoup

        soup = BeautifulSoup(html_content, 'html.parser')

        # é«˜äº®å¼•ç”¨æ–‡æœ¬
        if not citations:
            citations = []

        for citation in citations:
            if not citation or len(citation) < 10:
                continue

            # æŸ¥æ‰¾åŒ…å«å¼•ç”¨æ–‡æœ¬çš„æ‰€æœ‰æ–‡æœ¬èŠ‚ç‚¹
            for element in soup.find_all(string=lambda text: citation[:50] in text if text else False):
                try:
                    # æ›¿æ¢ä¸ºé«˜äº®ç‰ˆæœ¬
                    parent = element.parent
                    if parent:
                        highlighted = str(element).replace(
                            citation,
                            f'<mark style="background-color: yellow; padding: 2px;">{citation}</mark>'
                        )
                        element.replace_with(BeautifulSoup(highlighted, 'html.parser'))
                except:
                    continue

        # ä¿å­˜HTML
        report_path = self.reports_dir / f"task_{task_id}_report.html"
        report_path.write_text(str(soup), encoding='utf-8')

        return str(report_path)


# æµ‹è¯•å‡½æ•°
async def test_browser_agent():
    """æµ‹è¯•æµè§ˆå™¨ä»£ç†"""
    api_key = os.getenv('OPENAI_API_KEY')
    if not api_key:
        print("é”™è¯¯ï¼šæœªè®¾ç½® OPENAI_API_KEY")
        return

    agent = BrowserAgent(api_key=api_key, max_steps=5)

    result = await agent.execute_task(
        query="å›½å®¶å¯¹äºé¢„åˆ¶èœçš„å®šä¹‰æ˜¯ä»€ä¹ˆ",
        target_url="https://www.gov.cn",
        task_id=999
    )

    print("="*60)
    print("æµ‹è¯•ç»“æœ:")
    print("="*60)
    print(json.dumps(result, indent=2, ensure_ascii=False))


if __name__ == '__main__':
    logging.basicConfig(level=logging.INFO)
    asyncio.run(test_browser_agent())
