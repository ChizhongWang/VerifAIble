"""
train_with_new_data.py - ä½¿ç”¨æ–°æ•°æ®é›†è®­ç»ƒæ¨¡å‹ï¼Œæ”¯æŒåŠ¨æ€URLé…ç½®
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import json
import pickle
import random
from typing import Dict, List, Tuple
from collections import Counter
from tqdm import tqdm
from sklearn.model_selection import train_test_split

# ==================== é…ç½®ç±» ====================

class Config:
    """è®­ç»ƒé…ç½®"""
    # æ•°æ®ç›¸å…³
    data_file = 'training_data_v2.json'
    url_config_file = 'url_config.json'
    test_split = 0.2
    val_split = 0.1
    
    # æ¨¡å‹ç›¸å…³
    max_features = 5000  # è¯æ±‡è¡¨å¤§å°
    max_len = 60  # æœ€å¤§åºåˆ—é•¿åº¦
    embedding_dim = 128
    hidden_dim = 128
    num_filters = 100
    filter_sizes = [3, 4, 5]
    dropout_rate = 0.5
    
    # è®­ç»ƒç›¸å…³
    batch_size = 64
    num_epochs = 10
    learning_rate = 0.001
    patience = 15
    
    # è¾“å‡ºæ–‡ä»¶
    model_file = 'intent_model_v2.pth'
    vectorizer_file = 'vectorizer_v2.pkl'
    category_map_file = 'category_map.json'  # ç±»åˆ«æ˜ å°„ï¼Œä¸åŒ…å«URL


# ==================== æ•°æ®å¤„ç† ====================

class TextVectorizer:
    """æ–‡æœ¬å‘é‡åŒ–å™¨"""
    
    def __init__(self, max_features=5000, max_len=60):
        self.max_features = max_features
        self.max_len = max_len
        self.char2idx = {'<PAD>': 0, '<UNK>': 1, '<START>': 2, '<END>': 3}
        self.idx2char = {0: '<PAD>', 1: '<UNK>', 2: '<START>', 3: '<END>'}
        
    def fit(self, texts: List[str]):
        """æ„å»ºè¯æ±‡è¡¨"""
        char_counter = Counter()
        
        for text in texts:
            char_counter.update(list(text))
        
        most_common = char_counter.most_common(self.max_features - 4)
        
        for idx, (char, _) in enumerate(most_common, start=4):
            self.char2idx[char] = idx
            self.idx2char[idx] = char
            
        print(f"âœ… è¯æ±‡è¡¨å¤§å°: {len(self.char2idx)}")
        return self
    
    def transform(self, texts: List[str]) -> np.ndarray:
        """æ–‡æœ¬è½¬ç´¢å¼•åºåˆ—"""
        sequences = []
        
        for text in texts:
            seq = [self.char2idx.get(char, 1) for char in text[:self.max_len-2]]
            seq = [2] + seq + [3]  # æ·»åŠ å¼€å§‹å’Œç»“æŸæ ‡è®°
            
            if len(seq) < self.max_len:
                seq = seq + [0] * (self.max_len - len(seq))
            else:
                seq = seq[:self.max_len]
                
            sequences.append(seq)
            
        return np.array(sequences)
    
    def save(self, path: str):
        with open(path, 'wb') as f:
            pickle.dump(self, f)
    
    @staticmethod
    def load(path: str):
        with open(path, 'rb') as f:
            return pickle.load(f)


class IntentDataset(Dataset):
    """æ„å›¾è¯†åˆ«æ•°æ®é›†"""
    
    def __init__(self, sequences, labels):
        self.sequences = torch.LongTensor(sequences)
        self.labels = torch.LongTensor(labels)
        
    def __len__(self):
        return len(self.labels)
    
    def __getitem__(self, idx):
        return self.sequences[idx], self.labels[idx]


# ==================== æ¨¡å‹å®šä¹‰ ====================

class IntentClassifierV2(nn.Module):
    """å¢å¼ºç‰ˆæ„å›¾åˆ†ç±»å™¨"""
    
    def __init__(self, vocab_size, num_classes, config: Config):
        super(IntentClassifierV2, self).__init__()
        
        # Embeddingå±‚ï¼ˆå¸¦é¢„è®­ç»ƒåˆå§‹åŒ–ï¼‰
        self.embedding = nn.Embedding(vocab_size, config.embedding_dim, padding_idx=0)
        nn.init.xavier_uniform_(self.embedding.weight[4:])  # ä¸åˆå§‹åŒ–ç‰¹æ®Šæ ‡è®°
        
        # CNNåˆ†æ”¯
        self.convs = nn.ModuleList([
            nn.Sequential(
                nn.Conv1d(config.embedding_dim, config.num_filters, kernel_size=fs),
                nn.BatchNorm1d(config.num_filters),
                nn.ReLU(),
                nn.MaxPool1d(2)
            )
            for fs in config.filter_sizes
        ])
        
        # BiGRUåˆ†æ”¯
        self.bigru = nn.GRU(
            config.embedding_dim, 
            config.hidden_dim,
            num_layers=2,
            batch_first=True,
            bidirectional=True,
            dropout=config.dropout_rate if config.dropout_rate > 0 else 0
        )
        
        # æ³¨æ„åŠ›æœºåˆ¶
        self.attention = nn.Sequential(
            nn.Linear(config.hidden_dim * 2, config.hidden_dim),
            nn.Tanh(),
            nn.Linear(config.hidden_dim, 1, bias=False)
        )
        
        # Dropout
        self.dropout = nn.Dropout(config.dropout_rate)
        
        # åˆ†ç±»å™¨
        cnn_dim = len(config.filter_sizes) * config.num_filters
        gru_dim = config.hidden_dim * 2
        
        self.classifier = nn.Sequential(
            nn.Linear(cnn_dim + gru_dim, config.hidden_dim * 2),
            nn.BatchNorm1d(config.hidden_dim * 2),
            nn.ReLU(),
            nn.Dropout(config.dropout_rate),
            nn.Linear(config.hidden_dim * 2, config.hidden_dim),
            nn.ReLU(),
            nn.Dropout(config.dropout_rate / 2),
            nn.Linear(config.hidden_dim, num_classes)
        )
        
    def forward(self, x):
        # Embedding
        embedded = self.embedding(x)  # [batch, seq_len, emb_dim]
        
        # CNNç‰¹å¾
        embedded_t = embedded.transpose(1, 2)  # [batch, emb_dim, seq_len]
        cnn_features = []
        for conv in self.convs:
            conv_out = conv(embedded_t)
            pooled = torch.max(conv_out, dim=2)[0]
            cnn_features.append(pooled)
        cnn_out = torch.cat(cnn_features, dim=1)
        
        # GRUç‰¹å¾
        gru_out, _ = self.bigru(embedded)  # [batch, seq_len, hidden*2]
        
        # æ³¨æ„åŠ›åŠ æƒ
        att_weights = self.attention(gru_out)  # [batch, seq_len, 1]
        att_weights = torch.softmax(att_weights, dim=1)
        gru_weighted = torch.sum(gru_out * att_weights, dim=1)  # [batch, hidden*2]
        
        # ç‰¹å¾èåˆ
        combined = torch.cat([cnn_out, gru_weighted], dim=1)
        combined = self.dropout(combined)
        
        # åˆ†ç±»
        output = self.classifier(combined)
        
        return output


# ==================== è®­ç»ƒå™¨ ====================

class ModelTrainer:
    """æ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, config: Config):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {self.device}")
        
    def prepare_data(self):
        """å‡†å¤‡æ•°æ®"""
        print("\nğŸ“Š å‡†å¤‡æ•°æ®...")
        
        # åŠ è½½æ•°æ®
        with open(self.config.data_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # æå–æ–‡æœ¬å’Œç±»åˆ«
        texts = [item['text'] for item in data]
        categories = [item['category'] for item in data]
        
        # åˆ›å»ºç±»åˆ«åˆ°ç´¢å¼•çš„æ˜ å°„
        unique_categories = sorted(list(set(categories)))
        cat_to_idx = {cat: idx for idx, cat in enumerate(unique_categories)}
        idx_to_cat = {idx: cat for cat, idx in cat_to_idx.items()}
        
        # ä¿å­˜ç±»åˆ«æ˜ å°„ï¼ˆä¸åŒ…å«URLï¼‰
        with open(self.config.category_map_file, 'w', encoding='utf-8') as f:
            json.dump(idx_to_cat, f, ensure_ascii=False, indent=2)
        
        # è½¬æ¢ç±»åˆ«ä¸ºç´¢å¼•
        labels = [cat_to_idx[cat] for cat in categories]
        
        # åˆ’åˆ†æ•°æ®é›†
        X_temp, X_test, y_temp, y_test = train_test_split(
            texts, labels, test_size=self.config.test_split, 
            random_state=42, stratify=labels
        )
        
        X_train, X_val, y_train, y_val = train_test_split(
            X_temp, y_temp, test_size=self.config.val_split,
            random_state=42, stratify=y_temp
        )
        
        print(f"  è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
        print(f"  éªŒè¯é›†: {len(X_val)} æ ·æœ¬")
        print(f"  æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬")
        print(f"  ç±»åˆ«æ•°: {len(unique_categories)}")
        
        # åˆ›å»ºå‘é‡åŒ–å™¨
        self.vectorizer = TextVectorizer(
            max_features=self.config.max_features,
            max_len=self.config.max_len
        )
        self.vectorizer.fit(X_train)
        
        # å‘é‡åŒ–
        train_seq = self.vectorizer.transform(X_train)
        val_seq = self.vectorizer.transform(X_val)
        test_seq = self.vectorizer.transform(X_test)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_dataset = IntentDataset(train_seq, y_train)
        val_dataset = IntentDataset(val_seq, y_val)
        test_dataset = IntentDataset(test_seq, y_test)
        
        self.train_loader = DataLoader(
            train_dataset, batch_size=self.config.batch_size,
            shuffle=True, num_workers=2
        )
        self.val_loader = DataLoader(
            val_dataset, batch_size=self.config.batch_size,
            shuffle=False, num_workers=2
        )
        self.test_loader = DataLoader(
            test_dataset, batch_size=self.config.batch_size,
            shuffle=False, num_workers=2
        )
        
        self.num_classes = len(unique_categories)
        self.vocab_size = len(self.vectorizer.char2idx)
        
        return idx_to_cat
    
    def train(self):
        """è®­ç»ƒæ¨¡å‹"""
        print("\nğŸš€ å¼€å§‹è®­ç»ƒ...")
        
        # åˆ›å»ºæ¨¡å‹
        self.model = IntentClassifierV2(
            self.vocab_size, 
            self.num_classes, 
            self.config
        ).to(self.device)
        
        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        total_params = sum(p.numel() for p in self.model.parameters())
        print(f"  æ¨¡å‹å‚æ•°é‡: {total_params:,}")
        
        # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(self.model.parameters(), lr=self.config.learning_rate)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', patience=5, factor=0.5
        )
        
        # è®­ç»ƒå¾ªç¯
        best_val_loss = float('inf')
        best_val_acc = 0
        patience_counter = 0
        history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}
        
        for epoch in range(self.config.num_epochs):
            # è®­ç»ƒé˜¶æ®µ
            self.model.train()
            train_loss = 0
            train_correct = 0
            train_total = 0
            
            train_bar = tqdm(self.train_loader, desc=f'Epoch {epoch+1}/{self.config.num_epochs}')
            for inputs, labels in train_bar:
                inputs, labels = inputs.to(self.device), labels.to(self.device)
                
                optimizer.zero_grad()
                outputs = self.model(inputs)
                loss = criterion(outputs, labels)
                loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                
                optimizer.step()
                
                train_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                train_total += labels.size(0)
                train_correct += (predicted == labels).sum().item()
                
                train_bar.set_postfix({
                    'loss': f'{loss.item():.4f}',
                    'acc': f'{train_correct/train_total:.4f}'
                })
            
            # éªŒè¯é˜¶æ®µ
            self.model.eval()
            val_loss = 0
            val_correct = 0
            val_total = 0
            
            with torch.no_grad():
                for inputs, labels in self.val_loader:
                    inputs, labels = inputs.to(self.device), labels.to(self.device)
                    
                    outputs = self.model(inputs)
                    loss = criterion(outputs, labels)
                    
                    val_loss += loss.item()
                    _, predicted = torch.max(outputs.data, 1)
                    val_total += labels.size(0)
                    val_correct += (predicted == labels).sum().item()
            
            # è®¡ç®—å¹³å‡æŒ‡æ ‡
            avg_train_loss = train_loss / len(self.train_loader)
            avg_val_loss = val_loss / len(self.val_loader)
            train_acc = train_correct / train_total
            val_acc = val_correct / val_total
            
            # è®°å½•å†å²
            history['train_loss'].append(avg_train_loss)
            history['val_loss'].append(avg_val_loss)
            history['train_acc'].append(train_acc)
            history['val_acc'].append(val_acc)
            
            # å­¦ä¹ ç‡è°ƒæ•´
            scheduler.step(avg_val_loss)
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                best_val_loss = avg_val_loss
                self.save_model()
                patience_counter = 0
                print(f"  âœ… æœ€ä½³æ¨¡å‹å·²ä¿å­˜ (val_acc: {val_acc:.4f})")
            else:
                patience_counter += 1
            
            # æ‰“å°ç»“æœ
            if (epoch + 1) % 5 == 0 or patience_counter == 0:
                print(f"  Epoch {epoch+1}: "
                      f"train_loss={avg_train_loss:.4f}, train_acc={train_acc:.4f}, "
                      f"val_loss={avg_val_loss:.4f}, val_acc={val_acc:.4f}")
            
            # æ—©åœ
            if patience_counter >= self.config.patience:
                print(f"\nâ¹ï¸ æ—©åœ: éªŒè¯å‡†ç¡®ç‡å·²ç»{self.config.patience}è½®æ²¡æœ‰æ”¹å–„")
                break
        
        print(f"\nğŸ† æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f}")
        return history
    
    def test(self):
        """æµ‹è¯•æ¨¡å‹"""
        print("\nğŸ§ª æµ‹è¯•æ¨¡å‹...")
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        checkpoint = torch.load(self.config.model_file, map_location=self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model.eval()
        
        test_correct = 0
        test_total = 0
        
        with torch.no_grad():
            for inputs, labels in tqdm(self.test_loader, desc='Testing'):
                inputs, labels = inputs.to(self.device), labels.to(self.device)
                
                outputs = self.model(inputs)
                _, predicted = torch.max(outputs.data, 1)
                test_total += labels.size(0)
                test_correct += (predicted == labels).sum().item()
        
        test_acc = test_correct / test_total
        print(f"  æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.4f}")
        
        return test_acc
    
    def save_model(self):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'model_config': {
                'vocab_size': self.vocab_size,
                'num_classes': self.num_classes,
                'config': self.config.__dict__
            }
        }, self.config.model_file)
        
        # ä¿å­˜å‘é‡åŒ–å™¨
        self.vectorizer.save(self.config.vectorizer_file)


# ==================== é¢„æµ‹å™¨ï¼ˆæ”¯æŒåŠ¨æ€URLï¼‰ ====================

class DynamicIntentPredictor:
    """åŠ¨æ€æ„å›¾é¢„æµ‹å™¨ - æ”¯æŒè¿è¡Œæ—¶ä¿®æ”¹URL"""
    
    def __init__(self, model_file='intent_model_v2.pth',
                 vectorizer_file='vectorizer_v2.pkl',
                 category_map_file='category_map.json',
                 url_config_file='url_config.json'):
        
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # åŠ è½½å‘é‡åŒ–å™¨
        self.vectorizer = TextVectorizer.load(vectorizer_file)
        
        # åŠ è½½ç±»åˆ«æ˜ å°„
        with open(category_map_file, 'r', encoding='utf-8') as f:
            self.idx_to_cat = json.load(f)
            self.idx_to_cat = {int(k): v for k, v in self.idx_to_cat.items()}
        
        # åŠ è½½URLé…ç½®ï¼ˆå¯åŠ¨æ€ä¿®æ”¹ï¼‰
        self.load_url_config(url_config_file)
        
        # åŠ è½½æ¨¡å‹
        checkpoint = torch.load(model_file, map_location=self.device)
        model_config = checkpoint['model_config']
        config = Config()
        for k, v in model_config.get('config', {}).items():
            setattr(config, k, v)
        
        self.model = IntentClassifierV2(
            model_config['vocab_size'],
            model_config['num_classes'],
            config
        )
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model.to(self.device)
        self.model.eval()
        
    def load_url_config(self, url_config_file):
        """åŠ è½½æˆ–é‡æ–°åŠ è½½URLé…ç½®"""
        with open(url_config_file, 'r', encoding='utf-8') as f:
            self.url_config = json.load(f)
        print(f"âœ… URLé…ç½®å·²åŠ è½½: {url_config_file}")
    
    def predict(self, text: str, top_k: int = 3) -> List[Tuple[str, str, float]]:
        """
        é¢„æµ‹æ–‡æœ¬æ„å›¾
        è¿”å›: [(ç±»åˆ«, URL, ç½®ä¿¡åº¦), ...]
        """
        # å‘é‡åŒ–
        sequence = self.vectorizer.transform([text])
        inputs = torch.LongTensor(sequence).to(self.device)
        
        # é¢„æµ‹
        with torch.no_grad():
            outputs = self.model(inputs)
            probabilities = torch.softmax(outputs, dim=1)
            
            # è·å–top-k
            top_probs, top_indices = torch.topk(probabilities[0], min(top_k, len(self.idx_to_cat)))
            
            results = []
            for prob, idx in zip(top_probs.cpu().numpy(), top_indices.cpu().numpy()):
                category = self.idx_to_cat.get(int(idx), "unknown")
                url = self.url_config.get(category, "https://www.google.com/")
                results.append((category, url, float(prob)))
        
        return results
    
    def update_url(self, category: str, new_url: str):
        """åŠ¨æ€æ›´æ–°æŸä¸ªç±»åˆ«çš„URL"""
        if category in self.url_config:
            self.url_config[category] = new_url
            print(f"âœ… å·²æ›´æ–° {category} çš„URLä¸º: {new_url}")
        else:
            print(f"âš ï¸ ç±»åˆ« {category} ä¸å­˜åœ¨")


# ==================== ä¸»å‡½æ•° ====================

def main():
    """ä¸»è®­ç»ƒæµç¨‹"""
    print("="*60)
    print("ğŸš€ æ„å›¾è¯†åˆ«æ¨¡å‹è®­ç»ƒ V2.0")
    print("="*60)
    
    # åˆå§‹åŒ–é…ç½®
    config = Config()
    trainer = ModelTrainer(config)
    
    # å‡†å¤‡æ•°æ®
    idx_to_cat = trainer.prepare_data()
    
    # è®­ç»ƒæ¨¡å‹
    history = trainer.train()
    
    # æµ‹è¯•æ¨¡å‹
    test_acc = trainer.test()
    
    # å¿«é€Ÿæµ‹è¯•
    print("\nğŸ“ å¿«é€Ÿæµ‹è¯•:")
    predictor = DynamicIntentPredictor()
    
    test_queries = [
        "è´µå·èŒ…å°è‚¡ç¥¨æ€ä¹ˆæ ·",
        "å­—èŠ‚è·³åŠ¨å…¬å¸å¾…é‡",
        "ä¸ªäººæ‰€å¾—ç¨æœ€æ–°æ”¿ç­–",
        "æ¯”ç‰¹å¸ä»Šæ—¥è¡Œæƒ…",
        "Pythonæ€ä¹ˆå­¦"
    ]
    
    for query in test_queries:
        results = predictor.predict(query, top_k=2)
        print(f"\næŸ¥è¯¢: {query}")
        for cat, url, conf in results:
            print(f"  {cat:15s} â†’ {url:30s} (ç½®ä¿¡åº¦: {conf:.3f})")
    
    print("\nâœ… è®­ç»ƒå®Œæˆ!")
    print(f"ğŸ“ æ¨¡å‹æ–‡ä»¶: {config.model_file}")
    print(f"ğŸ“ å‘é‡åŒ–å™¨: {config.vectorizer_file}")
    print(f"ğŸ“ ç±»åˆ«æ˜ å°„: {config.category_map_file}")
    print(f"ğŸ“ URLé…ç½®: {config.url_config_file}")
    print("\nğŸ’¡ æç¤º: æ‚¨å¯ä»¥éšæ—¶ä¿®æ”¹ url_config.json æ¥æ›´æ”¹URLæ˜ å°„ï¼Œæ— éœ€é‡æ–°è®­ç»ƒ!")


if __name__ == "__main__":
    main()